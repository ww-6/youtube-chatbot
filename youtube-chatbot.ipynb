{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ww-6/youtube-chatbot/blob/main/youtube-chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSe0VTTc6eWO"
      },
      "source": [
        "# YouTube Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G5l5uag6iCN"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U langchain==0.1.0 langchain-community==0.0.9 sentence-transformers==2.3.0 \\\n",
        "gradio yt_dlp jq==1.6.0 chromadb==0.4.22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HTsK_hF3PYfL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain.memory.buffer_window import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from langchain.chains import (\n",
        "    LLMChain,\n",
        "    StuffDocumentsChain,\n",
        "    MapReduceDocumentsChain,\n",
        "    ReduceDocumentsChain,\n",
        ")\n",
        "\n",
        "from gradio_client import Client\n",
        "import gradio as gr\n",
        "import yt_dlp\n",
        "import json\n",
        "import gc\n",
        "import datetime\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGnYb4fhPYfL"
      },
      "source": [
        "## 1. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDbCV-HNPYfM"
      },
      "outputs": [],
      "source": [
        "whisper_jax_api = \"https://sanchit-gandhi-whisper-jax.hf.space/\"\n",
        "whisper_jax = Client(whisper_jax_api)\n",
        "\n",
        "\n",
        "def transcribe_audio(audio_path, task=\"transcribe\", return_timestamps=True) -> str:\n",
        "    text, runtime = whisper_jax.predict(\n",
        "        audio_path,\n",
        "        task,\n",
        "        return_timestamps,\n",
        "        api_name=\"/predict_1\",\n",
        "    )\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "izkQmZqkXWO_"
      },
      "outputs": [],
      "source": [
        "def format_whisper_jax_output(\n",
        "    whisper_jax_output: str, max_duration: int = 60\n",
        ") -> list[dict]:\n",
        "    \"\"\"Whisper JAX outputs are in the format\n",
        "    '[00:00.000 -> 00:00.000] text\\n[00:00.000 -> 00:00.000] text'.\n",
        "\n",
        "    Returns a list of dict with keys 'start', 'end', 'text'\n",
        "    The segments from whisper jax output are merged to form paragraphs.\n",
        "\n",
        "    `max_duration` controls how many seconds of the audio's transcripts are merged\n",
        "\n",
        "    For example, if `max_duration`=60, in the final output, each segment is roughly\n",
        "    60 seconds.\n",
        "    \"\"\"\n",
        "\n",
        "    final_output = []\n",
        "    max_duration = datetime.timedelta(seconds=max_duration)\n",
        "    segments = whisper_jax_output.split(\"\\n\")\n",
        "    current_start = datetime.datetime.strptime(\"00:00\", \"%M:%S\")\n",
        "    current_text = \"\"\n",
        "\n",
        "    for i, seg in enumerate(segments):\n",
        "        text = seg.split(\"]\")[-1].strip()\n",
        "        current_text += \" \" + text\n",
        "\n",
        "        # Sometimes whisper jax returns None for timestamp\n",
        "        try:\n",
        "            end = datetime.datetime.strptime(seg[14:19], \"%M:%S\")\n",
        "        except ValueError:\n",
        "            end = current_start + max_duration\n",
        "\n",
        "        if i == len(segments) - 1:\n",
        "            final_output.append(\n",
        "                {\n",
        "                    \"start\": current_start.strftime(\"%H:%M:%S\"),\n",
        "                    \"end\": end.strftime(\"%H:%M:%S\"),\n",
        "                    \"text\": current_text.strip(),\n",
        "                }\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            if end - current_start >= max_duration and current_text[-1] == \".\":\n",
        "                # If we have exceeded max duration, check whether we have\n",
        "                # reached the end of a sentence. If not, keep merging.\n",
        "                final_output.append(\n",
        "                    {\n",
        "                        \"start\": current_start.strftime(\"%H:%M:%S\"),\n",
        "                        \"end\": end.strftime(\"%H:%M:%S\"),\n",
        "                        \"text\": current_text.strip(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Update current start and text\n",
        "                current_start = end\n",
        "                current_text = \"\"\n",
        "\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v3HLNmCVXaS8"
      },
      "outputs": [],
      "source": [
        "def yt_audio_to_text(url: str, max_duration: int = 60):\n",
        "    \"\"\"Given a YouTube url, download audio and transcribe it to text. Reformat\n",
        "    the output from Whisper JAX and save the final result in a json file.\n",
        "    \"\"\"\n",
        "\n",
        "    progress = gr.Progress()\n",
        "    progress(0.1)\n",
        "\n",
        "    with yt_dlp.YoutubeDL(\n",
        "        {\"extract_audio\": True, \"format\": \"bestaudio\", \"outtmpl\": \"audio.mp3\"}\n",
        "    ) as video:\n",
        "        info_dict = video.extract_info(url, download=False)\n",
        "        global video_title\n",
        "        video_title = info_dict[\"title\"]\n",
        "        video.download(url)\n",
        "\n",
        "    progress(0.4)\n",
        "    audio_file = \"audio.mp3\"\n",
        "\n",
        "    result = transcribe_audio(audio_file, return_timestamps=True)\n",
        "    progress(0.7)\n",
        "\n",
        "    result = format_whisper_jax_output(result, max_duration=max_duration)\n",
        "    progress(0.9)\n",
        "\n",
        "    with open(\"audio.json\", \"w\") as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "    os.remove(audio_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syFJmVkJPYfM"
      },
      "source": [
        "## 2. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5jjqkPjDPYfN"
      },
      "outputs": [],
      "source": [
        "def metadata_func(record: dict, metadata: dict) -> dict:\n",
        "    \"\"\"This function is used to tell the Langchain loader the keys that\n",
        "    contain metadata and extract them.\n",
        "    \"\"\"\n",
        "    metadata[\"start\"] = record.get(\"start\")\n",
        "    metadata[\"end\"] = record.get(\"end\")\n",
        "    metadata[\"source\"] = metadata[\"start\"] + \" -> \" + metadata[\"end\"]\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    loader = JSONLoader(\n",
        "        file_path=\"audio.json\",\n",
        "        jq_schema=\".[]\",\n",
        "        content_key=\"text\",\n",
        "        metadata_func=metadata_func,\n",
        "    )\n",
        "\n",
        "    data = loader.load()\n",
        "    os.remove(\"audio.json\")\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afNgaeFDPYfN"
      },
      "source": [
        "## 3. Create embeddings and vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e67FPeyiPYfN"
      },
      "outputs": [],
      "source": [
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embedding_model_kwargs = {\"device\": device}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name, model_kwargs=embedding_model_kwargs\n",
        ")\n",
        "\n",
        "\n",
        "def create_vectordb(data, n_retrieved_docs: int, collection_name=\"YouTube\"):\n",
        "    \"\"\"Returns a retriever which is used to fetch relevant documents from\n",
        "    the vector database.\n",
        "\n",
        "    `n_retrieved_docs` is the number of retrieved documents.\n",
        "    \"\"\"\n",
        "\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=data, embedding=embeddings, collection_name=collection_name\n",
        "    )\n",
        "    n_docs = len(vectordb.get()[\"ids\"])\n",
        "    retriever = vectordb.as_retriever(\n",
        "        search_type=\"mmr\", search_kwargs={\"k\": n_retrieved_docs, \"fetch_k\": n_docs}\n",
        "    )\n",
        "\n",
        "    return retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9h-t8E9PYfO"
      },
      "source": [
        "## 4. Load LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkoefkhBPYfP"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"max_new_tokens\": 1000})\n",
        "llm.client.api_url = 'https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdJw1fWT5qsv"
      },
      "source": [
        "## 5. Summarisation\n",
        "We will use the map-reduce method for summarisation. The documents will be first summarised individually (map step). Then their summaries are combined and reduced further to give a single global summary (reduce step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rm7hrfpr5qsv"
      },
      "outputs": [],
      "source": [
        "# Map\n",
        "map_template = \"\"\"Summarise the following text:\n",
        "{docs}\n",
        "\n",
        "Answer:\"\"\"\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EHc4YJI75qsw"
      },
      "outputs": [],
      "source": [
        "# Reduce\n",
        "reduce_template = \"\"\"The following is a set of summaries:\n",
        "{docs}\n",
        "\n",
        "Take these and distill it into a final, consolidated summary of the main themes \\\n",
        "in 150 words or less.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to llm\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl8wyioE5qsw"
      },
      "source": [
        "After summarising individual documents, the combined summaries could still exceed the max tokens. In that case, we pass in the summaries in batches and create batched summaries. Once all the batched summaries combined are less than the max tokens, we pass them all to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8oV56QIk5qsx"
      },
      "outputs": [],
      "source": [
        "# Combines and iteravely reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=4000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHnPQFnZ5qsy"
      },
      "source": [
        "Finally, we combine our map and reduce chains into one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nxY3cWuD5qsy"
      },
      "outputs": [],
      "source": [
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        ")\n",
        "\n",
        "\n",
        "def get_summary(documents) -> str:\n",
        "    summary = map_reduce_chain.invoke(documents, return_only_outputs=True)\n",
        "    return summary[\"output_text\"].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6BSGORfjo-h"
      },
      "source": [
        "## 6. Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQOv-Y0AnJSq"
      },
      "source": [
        "### 6.1 Contextualising the question\n",
        "The latest user question may make reference to information in the chat history but we don't want to use the entire history to search for answer in the database because not all of the information is relevant. We want to reformulate the question such that it contains only the relevant information, and can be understood without the chat history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5Tix4I4iPYfQ"
      },
      "outputs": [],
      "source": [
        "contextualise_q_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Given a chat history and the latest user question \\\n",
        "    which might reference the chat history, formulate a \\\n",
        "    standalone question that can be understood without \\\n",
        "    the chat history. Do NOT answer the question, just \\\n",
        "    reformulate it if needed and otherwise return it as is.\n",
        "\n",
        "    Chat history: {chat_history}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "contextualise_q_chain = contextualise_q_prompt | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgFkxlT_-36Y"
      },
      "source": [
        "Let's test our `contextualise_q_chain` and see if it does what we want. Suppose we have the following conversation:\n",
        "    \n",
        "> Human: What is the capital city of Australia?\n",
        "\n",
        "> AI: Canberra.\n",
        "\n",
        "> Human: How far is it from Sydney?\n",
        "\n",
        "We know that \"it\" here refers to Canberra. Let's see if the LLM can rephrase the question to include Canberra in the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "849hn8TA-ytz",
        "outputId": "ebe77a1b-c916-491d-8dfe-8e5f969cbe34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    What is the distance between Sydney and Canberra?\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "chat_history = []\n",
        "first_question = \"What is the capital city of Australia?\"\n",
        "ai_msg = \"Canberra\"\n",
        "chat_history.extend([HumanMessage(content=first_question), AIMessage(content=ai_msg)])\n",
        "\n",
        "second_question = \"How far is it from Sydney?\"\n",
        "answer = contextualise_q_chain.invoke(\n",
        "    {\"question\": second_question, \"chat_history\": chat_history}\n",
        ")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C65d6aO5ZQT8"
      },
      "source": [
        "### 6.2 Standalone question chain\n",
        "Reformulating the question takes time and not all questions need contextualising. To speed up the process, we add a sub-chain which determines whether a question needs contextualising or not. If the question is a standalone question, then we can use the user input directly without modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SwyWJz7QTulv"
      },
      "outputs": [],
      "source": [
        "standalone_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Given a chat history and the latest user question, \\\n",
        "    identify whether the question is a standalone question \\\n",
        "    or the question references the chat history. Answer 'yes' \\\n",
        "    if the question is a standalone question, and 'no' if the \\\n",
        "    question references the chat history. Do not answer \\\n",
        "    anything other than 'yes' or 'no'.\n",
        "\n",
        "    Chat history:\n",
        "    {chat_history}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "def format_output(answer: str) -> str:\n",
        "    \"\"\"All lower case and remove all whitespace to ensure\n",
        "    that the answer given by the LLM is either 'yes' or 'no'.\n",
        "    \"\"\"\n",
        "    return \"\".join(answer.lower().split())\n",
        "\n",
        "\n",
        "standalone_chain = standalone_prompt | llm | format_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpzjL-qwAntr"
      },
      "source": [
        "Let's test the `standalone_chain` using the same example as before. Recall that the question \"How far is it from Sydney?\" refers to the answer given by the AI so it is not a standalone question. Let's see if the LLM can pick up on that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ISIXJ6TAk33",
        "outputId": "41336619-e7da-4c01-857b-d5abe821b336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "answer = standalone_chain.invoke(\n",
        "    {\"question\": second_question, \"chat_history\": chat_history}\n",
        ")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyAN4SocnNm9"
      },
      "source": [
        "### 6.3 Q&A chain\n",
        "Finally, we can build our Q&A chain. The process goes as follows:\n",
        "\n",
        "\n",
        "1.   Check whether the latest user question needs contextualising or not, using the `standalone_chain`.\n",
        "2.   If the question is a standalone question, use it to retrieve documents from the database. Otherwise, reformulate the question using `contextualise_q_chain` to get a contextualised question and use it to retrieve documents from the database.\n",
        "3.   Pass the retrieved documents as `context`, together with the contextualised question to the LLM to receive an answer.\n",
        "\n",
        "\n",
        "We do not want the LLM to use outside knowledge so we tell the LLM that it can only use the information given in `context` to answer the question.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-t8Rw-gtPYfR"
      },
      "outputs": [],
      "source": [
        "qa_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant for question-answering tasks. \\\n",
        "    ONLY use the following context to answer the question. \\\n",
        "    Do NOT answer with information that is not contained in \\\n",
        "    the context. If you don't know the answer, just say:\\\n",
        "    \"Sorry, I cannot find the answer to that question in the video.\"\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Dxnf--riyY12"
      },
      "outputs": [],
      "source": [
        "class YouTubeChatbot:\n",
        "    instance_count = 0\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_sources: int = 3,\n",
        "        n_retrieved_docs: int = 5,\n",
        "        timestamp_interval: datetime.timedelta = datetime.timedelta(minutes=2),\n",
        "        memory: int = 5,\n",
        "    ):\n",
        "        YouTubeChatbot.instance_count += 1\n",
        "        self.chatbot_id = YouTubeChatbot.instance_count\n",
        "        self.n_sources = n_sources\n",
        "        self.n_retrieved_docs = n_retrieved_docs\n",
        "        self.timestamp_interval = timestamp_interval\n",
        "        self.chat_history = ConversationBufferWindowMemory(k=memory)\n",
        "        self.retriever = None\n",
        "        self.qa_chain = None\n",
        "\n",
        "\n",
        "    def format_docs(self, docs: list) -> str:\n",
        "        \"\"\"Combine documents into a single string which will be included\n",
        "        in the prompt given to the LLM.\n",
        "        \"\"\"\n",
        "        self.sources = [doc.metadata[\"start\"] for doc in docs]\n",
        "\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "    def standalone_question(self, input_: dict) -> str:\n",
        "        \"\"\"If the question is a not a standalone question,\n",
        "        run contextualise_q_chain.\n",
        "        \"\"\"\n",
        "        if input_[\"standalone\"] == \"yes\":\n",
        "            return contextualise_q_chain\n",
        "        else:\n",
        "            return input_[\"question\"]\n",
        "\n",
        "\n",
        "    def format_answer(self, answer: str) -> str:\n",
        "        \"\"\"Add timestamps to answers.\n",
        "        \"\"\"\n",
        "        if \"cannot find the answer\" in answer:\n",
        "            return answer.strip()\n",
        "        else:\n",
        "            timestamps = self.filter_timestamps()\n",
        "            answer_with_sources = (\n",
        "                answer.strip() + \" You can find more information \"\n",
        "                \"at these timestamps: {}.\".format(\", \".join(timestamps))\n",
        "            )\n",
        "            return answer_with_sources\n",
        "\n",
        "\n",
        "    def filter_timestamps(self) -> list[str]:\n",
        "        \"\"\"Returns a list of timestamps with length less or\n",
        "        equal to `n_sources`. The timestamps are at least an\n",
        "        `timestamp_interval` apart. This prevents returning\n",
        "        a list of timestamps that are too close together.\n",
        "        \"\"\"\n",
        "\n",
        "        filtered_timestamps = np.array(\n",
        "            [datetime.datetime.strptime(self.sources[0], \"%H:%M:%S\")]\n",
        "        )\n",
        "\n",
        "        i = 1\n",
        "\n",
        "        while len(filtered_timestamps) < self.n_sources:\n",
        "            try:\n",
        "                new_timestamp = datetime.datetime.strptime(self.sources[i], \"%H:%M:%S\")\n",
        "            except IndexError:\n",
        "                break\n",
        "\n",
        "            absolute_time_difference = abs(new_timestamp - filtered_timestamps)\n",
        "\n",
        "            if all(absolute_time_difference >= self.timestamp_interval):\n",
        "                filtered_timestamps = np.append(filtered_timestamps, new_timestamp)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        filtered_timestamps = [\n",
        "            timestamp.strftime(\"%H:%M:%S\") for timestamp in filtered_timestamps\n",
        "        ]\n",
        "        filtered_timestamps.sort()\n",
        "\n",
        "        return filtered_timestamps\n",
        "\n",
        "\n",
        "    def process_video(self, url: str, data=None, retriever=None):\n",
        "        \"\"\"Given a YouTube URL, transcribe YouTube audio to text.\n",
        "        Then set up the vector database.\n",
        "        \"\"\"\n",
        "        yt_audio_to_text(url)\n",
        "        data = load_data()\n",
        "\n",
        "        if retriever is not None:\n",
        "            # If we already have documents in the vector store, delete them.\n",
        "            ids = retriever.vectorstore.get()[\"ids\"]\n",
        "            retriever.vectorstore.delete(ids)\n",
        "\n",
        "        retriever = create_vectordb(\n",
        "            data, self.n_retrieved_docs,\n",
        "            collection_name=f\"Chatbot{self.chatbot_id}\"\n",
        "        )\n",
        "\n",
        "        return url, data, retriever\n",
        "\n",
        "\n",
        "    def setup_qa_chain(self, retriever, qa_chain=None):\n",
        "        qa_chain = (\n",
        "            RunnablePassthrough.assign(standalone=standalone_chain)\n",
        "            | {\n",
        "                \"question\": self.standalone_question,\n",
        "                \"context\": self.standalone_question | retriever | self.format_docs,\n",
        "            }\n",
        "            | qa_prompt\n",
        "            | llm\n",
        "        )\n",
        "\n",
        "        return retriever, qa_chain\n",
        "\n",
        "\n",
        "    def setup_chatbot(self, url: str):\n",
        "        _, _, self.retriever = self.process_video(url=url, retriever=self.retriever)\n",
        "        _, self.qa_chain = self.setup_qa_chain(retriever=self.retriever)\n",
        "\n",
        "\n",
        "    def get_answer(self, question: str) -> str:\n",
        "        try:\n",
        "            ai_msg = self.qa_chain.invoke(\n",
        "                {\"question\": question, \"chat_history\": self.chat_history}\n",
        "            )\n",
        "        except AttributeError:\n",
        "            raise AttributeError(\n",
        "                \"You haven't setup the chatbot yet. \"\n",
        "                \"Setup the chatbot by calling the \"\n",
        "                \"instance method `setup_chatbot`.\"\n",
        "            )\n",
        "\n",
        "        self.chat_history.save_context({\"question\": question}, {\"answer\": ai_msg})\n",
        "\n",
        "        answer = self.format_answer(ai_msg)\n",
        "\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osx_GOy6zt9O"
      },
      "source": [
        "### 6.4 Test our chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hat_DXPoPYfR"
      },
      "outputs": [],
      "source": [
        "chatbot = YouTubeChatbot(\n",
        "    n_sources=3,\n",
        "    n_retrieved_docs=5,\n",
        "    timestamp_interval=datetime.timedelta(minutes=2),\n",
        "    memory=5,\n",
        ")\n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=SZorAJ4I-sA\"\n",
        "chatbot.setup_chatbot(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDyc7EeVzzjj"
      },
      "source": [
        "#### 6.4.1 Test 1: A question that cannot be answered by the video alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1fTE8vfPYfR",
        "outputId": "df95dea5-9bfb-40d1-f4a3-d8e036de6f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sorry, I cannot find the answer to that question in the video.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the capital city of Australia?\"\n",
        "answer = chatbot.get_answer(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckevA0uW0Qsy"
      },
      "source": [
        "#### 6.4.2 Test 2: A question that can be answered by the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc7_m5u5PYfS",
        "outputId": "3369c734-67f0-4d56-f030-f8ff53da93bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers improved upon RNN in several ways. First, they were able to efficiently parallelize, which meant that with the right hardware, they could train some really big models. This allowed them to handle large sequences of text, like long paragraphs or essays, without forgetting what happened in the beginning. Second, transformers were easier to train than RNNs because they processed words sequentially, which meant that they could paralyze well and speed up training. Finally, transformers were able to build up an internal representation or understanding of language automatically as they analyzed tons of text data, which made them useful for a variety of language tasks. You can find more information at these timestamps: 00:00:00, 00:02:07, 00:05:12.\n"
          ]
        }
      ],
      "source": [
        "question = \"In what ways did transformers improve upon RNN?\"\n",
        "answer = chatbot.get_answer(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc90erEO1oyo"
      },
      "outputs": [],
      "source": [
        "del chatbot\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFHOyf8HPYfS"
      },
      "source": [
        "## 7. Web app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UNvljS1mlBiZ"
      },
      "outputs": [],
      "source": [
        "class YouTubeChatbotApp(YouTubeChatbot):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_sources: int,\n",
        "        n_retrieved_docs: int,\n",
        "        timestamp_interval: datetime.timedelta,\n",
        "        memory: int,\n",
        "        default_youtube_url: str,\n",
        "    ):\n",
        "        super().__init__(n_sources, n_retrieved_docs, timestamp_interval, memory)\n",
        "\n",
        "        self.default_youtube_url = default_youtube_url\n",
        "        self.memory = memory\n",
        "        self.chat_history = None\n",
        "        self.data = None\n",
        "        self.retriever = None\n",
        "        self.qa_chain = None\n",
        "\n",
        "        # Gradio components\n",
        "        self.url_input = None\n",
        "        self.url_button = None\n",
        "        self.app_chat_history = None\n",
        "        self.chatbot = None\n",
        "        self.user_input = None\n",
        "        self.clear_button = None\n",
        "\n",
        "    def greet(self, data, app_chat_history) -> dict:\n",
        "        \"\"\"Summarise the video and greet the user.\n",
        "        \"\"\"\n",
        "        summary_message = f'Here is a summary of the video \"{video_title}\":'\n",
        "        app_chat_history.append((None, summary_message))\n",
        "\n",
        "        summary = get_summary(data)\n",
        "        self.data = gr.State(None)\n",
        "        app_chat_history.append((None, summary))\n",
        "\n",
        "        greeting_message = (\n",
        "            \"You can ask me anything about the video. \" \"I will do my best to answer!\"\n",
        "        )\n",
        "        app_chat_history.append((None, greeting_message))\n",
        "\n",
        "        return {self.app_chat_history: app_chat_history, self.chatbot: app_chat_history}\n",
        "\n",
        "    def question(self, user_question: str, app_chat_history) -> dict:\n",
        "        \"\"\"Display the question asked by the user in the chat window,\n",
        "        and delete from the input textbox.\n",
        "        \"\"\"\n",
        "        app_chat_history.append((user_question, None))\n",
        "        return {\n",
        "            self.user_input: \"\",\n",
        "            self.app_chat_history: app_chat_history,\n",
        "            self.chatbot: app_chat_history,\n",
        "        }\n",
        "\n",
        "    def respond(self, qa_chain, chat_history, app_chat_history) -> dict:\n",
        "        \"\"\"Respond to user's latest question\"\"\"\n",
        "        question = app_chat_history[-1][0]\n",
        "\n",
        "        try:\n",
        "            ai_msg = qa_chain.invoke(\n",
        "                {\"question\": question, \"chat_history\": chat_history}\n",
        "            )\n",
        "        except AttributeError:\n",
        "            raise gr.Error(\n",
        "                \"You need to process the video \" \"first by pressing the `Go` button.\"\n",
        "            )\n",
        "\n",
        "        chat_history.save_context({\"question\": question}, {\"answer\": ai_msg})\n",
        "\n",
        "        answer = self.format_answer(ai_msg)\n",
        "\n",
        "        app_chat_history.append((None, answer))\n",
        "\n",
        "        return {\n",
        "            self.qa_chain: qa_chain,\n",
        "            self.chat_history: chat_history,\n",
        "            self.app_chat_history: app_chat_history,\n",
        "            self.chatbot: app_chat_history,\n",
        "        }\n",
        "\n",
        "    def clear_chat_history(self, chat_history, app_chat_history):\n",
        "        chat_history.clear()\n",
        "        app_chat_history = []\n",
        "        return {\n",
        "            self.chat_history: chat_history,\n",
        "            self.app_chat_history: app_chat_history,\n",
        "            self.chatbot: app_chat_history,\n",
        "        }\n",
        "\n",
        "    def launch(self, **kwargs):\n",
        "        with gr.Blocks() as demo:\n",
        "            self.chat_history = gr.State(ConversationBufferWindowMemory(k=self.memory))\n",
        "            self.app_chat_history = gr.State([])\n",
        "            self.data = gr.State()\n",
        "            self.retriever = gr.State()\n",
        "            self.qa_chain = gr.State()\n",
        "\n",
        "            # App structure\n",
        "            with gr.Row():\n",
        "                self.url_input = gr.Textbox(\n",
        "                    value=self.default_youtube_url, label=\"YouTube URL\", scale=5\n",
        "                )\n",
        "                self.url_button = gr.Button(value=\"Go\", scale=1)\n",
        "\n",
        "            self.chatbot = gr.Chatbot()\n",
        "            self.user_input = gr.Textbox(label=\"Ask a question:\")\n",
        "            self.clear_button = gr.Button(value=\"Clear\")\n",
        "\n",
        "\n",
        "            # App actions\n",
        "\n",
        "            # When a new url is given, clear past chat history and process\n",
        "            # the new video. Set up the Q&A chain with the new video's data.\n",
        "            # Provide a summary of the new video.\n",
        "            self.url_button.click(\n",
        "                self.clear_chat_history,\n",
        "                inputs=[self.chat_history, self.app_chat_history],\n",
        "                outputs=[self.chat_history, self.app_chat_history, self.chatbot],\n",
        "                trigger_mode=\"once\",\n",
        "            ).then(\n",
        "                self.process_video,\n",
        "                inputs=[self.url_input, self.data, self.retriever],\n",
        "                outputs=[self.url_input, self.data, self.retriever],\n",
        "            ).then(\n",
        "                self.setup_qa_chain,\n",
        "                inputs=[self.retriever, self.qa_chain],\n",
        "                outputs=[self.retriever, self.qa_chain],\n",
        "            ).then(\n",
        "                self.greet,\n",
        "                inputs=[self.data, self.app_chat_history],\n",
        "                outputs=[self.app_chat_history, self.chatbot],\n",
        "            )\n",
        "\n",
        "            # When a user asks a question, display the question in the chat\n",
        "            # window and remove it from the text input area. Then respond\n",
        "            # with the Q&A chain.\n",
        "            self.user_input.submit(\n",
        "                self.question,\n",
        "                inputs=[self.user_input, self.app_chat_history],\n",
        "                outputs=[self.user_input, self.app_chat_history, self.chatbot],\n",
        "                queue=False,\n",
        "            ).then(\n",
        "                self.respond,\n",
        "                inputs=[self.qa_chain, self.chat_history, self.app_chat_history],\n",
        "                outputs=[\n",
        "                    self.qa_chain,\n",
        "                    self.chat_history,\n",
        "                    self.app_chat_history,\n",
        "                    self.chatbot,\n",
        "                ],\n",
        "            )\n",
        "\n",
        "            # When the `Clear` button is clicked, clear the chat history from\n",
        "            # the chat window.\n",
        "            self.clear_button.click(\n",
        "                self.clear_chat_history,\n",
        "                inputs=[self.chat_history, self.app_chat_history],\n",
        "                outputs=[self.chat_history, self.app_chat_history, self.chatbot],\n",
        "                queue=False,\n",
        "            )\n",
        "\n",
        "        demo.launch(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov8eA2-7u3LJ"
      },
      "outputs": [],
      "source": [
        "app = YouTubeChatbotApp(\n",
        "    n_sources=3,\n",
        "    n_retrieved_docs=5,\n",
        "    timestamp_interval=datetime.timedelta(minutes=2),\n",
        "    memory=5,\n",
        "    default_youtube_url=\"https://www.youtube.com/watch?v=SZorAJ4I-sA\",\n",
        ")\n",
        "\n",
        "app.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZvDzsT1oTlio"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
