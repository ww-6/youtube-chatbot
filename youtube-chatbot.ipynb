{
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ww-6/youtube-chatbot/blob/main/youtube-chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YouTube Chatbot"
      ],
      "metadata": {
        "id": "JSe0VTTc6eWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes langchain chromadb jq \\\n",
        "sentence-transformers gradio yt_dlp pydantic"
      ],
      "metadata": {
        "id": "2G5l5uag6iCN",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:14:27.444322Z",
          "iopub.execute_input": "2024-01-06T07:14:27.445347Z",
          "iopub.status.idle": "2024-01-06T07:16:23.454649Z",
          "shell.execute_reply.started": "2024-01-06T07:14:27.445304Z",
          "shell.execute_reply": "2024-01-06T07:16:23.453516Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import GenerationConfig\n",
        "from transformers import pipeline\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import JSONLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain.chains import LLMChain, StuffDocumentsChain, MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "\n",
        "import yt_dlp\n",
        "import json\n",
        "import gc\n",
        "import gradio as gr\n",
        "from gradio_client import Client\n",
        "import datetime\n",
        "from time import time\n",
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "HTsK_hF3PYfL",
        "execution": {
          "iopub.status.busy": "2024-01-06T08:18:13.731721Z",
          "iopub.execute_input": "2024-01-06T08:18:13.732470Z",
          "iopub.status.idle": "2024-01-06T08:18:13.740088Z",
          "shell.execute_reply.started": "2024-01-06T08:18:13.732422Z",
          "shell.execute_reply": "2024-01-06T08:18:13.739115Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prepare data"
      ],
      "metadata": {
        "id": "DGnYb4fhPYfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_jax_api = 'https://sanchit-gandhi-whisper-jax.hf.space/'\n",
        "whisper_jax = Client(whisper_jax_api)\n",
        "\n",
        "def transcribe_audio(audio_path, task='transcribe', return_timestamps=True):\n",
        "    text, runtime = whisper_jax.predict(\n",
        "        audio_path,\n",
        "        task,\n",
        "        return_timestamps,\n",
        "        api_name='/predict_1',\n",
        "    )\n",
        "    return text"
      ],
      "metadata": {
        "id": "pDbCV-HNPYfM",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:20:30.883235Z",
          "iopub.execute_input": "2024-01-06T07:20:30.884081Z",
          "iopub.status.idle": "2024-01-06T07:20:38.477313Z",
          "shell.execute_reply.started": "2024-01-06T07:20:30.884046Z",
          "shell.execute_reply": "2024-01-06T07:20:38.476491Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_whisper_jax_output(whisper_jax_output: str, max_duration: int=60) -> list:\n",
        "\n",
        "    '''\n",
        "    Returns a list of dict with keys 'start', 'end', 'text'\n",
        "    The segments from whisper jax output are merged to form paragraphs.\n",
        "\n",
        "    `max_duration` controls how many seconds of the audio's transcripts are merged\n",
        "\n",
        "    For example, if `max_duration`=60, in the final output, each segment is roughly\n",
        "    60 seconds.\n",
        "    '''\n",
        "\n",
        "    final_output = []\n",
        "    max_duration = datetime.timedelta(seconds=max_duration)\n",
        "    segments = whisper_jax_output.split('\\n')\n",
        "    current_start = datetime.datetime.strptime('00:00', '%M:%S')\n",
        "    current_text = ''\n",
        "\n",
        "    for i, seg in enumerate(segments):\n",
        "\n",
        "        text = seg.split(']')[-1].strip()\n",
        "        end = datetime.datetime.strptime(seg[14:19], '%M:%S')\n",
        "\n",
        "        if (end - current_start > max_duration) or (i == len(segments)-1):\n",
        "            # If we have exceeded max duration or\n",
        "            # at the last segment, stop merging\n",
        "            # and append to final_output\n",
        "            current_text += text\n",
        "            final_output.append({'start': current_start.strftime('%H:%M:%S'),\n",
        "                                 'end': end.strftime('%H:%M:%S'),\n",
        "                                 'text': current_text\n",
        "                                })\n",
        "\n",
        "            # Update current start and text\n",
        "            current_start = end\n",
        "            current_text = ''\n",
        "\n",
        "        else:\n",
        "            # If we have not exceeded max duration,\n",
        "            # keep merging.\n",
        "            current_text += text\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "izkQmZqkXWO_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_number = 1\n",
        "def yt_audio_to_text(url: str,\n",
        "                     max_duration: int = 60\n",
        "                    ):\n",
        "\n",
        "    global audio_file_number\n",
        "    global progress\n",
        "    progress = gr.Progress()\n",
        "    progress(0.1)\n",
        "\n",
        "    with yt_dlp.YoutubeDL({'extract_audio': True,\n",
        "                           'format': 'bestaudio',\n",
        "                           'outtmpl': f'{audio_file_number}.mp3'}) as video:\n",
        "\n",
        "        info_dict = video.extract_info(url, download=False)\n",
        "        global video_title\n",
        "        video_title = info_dict['title']\n",
        "        video.download(url)\n",
        "\n",
        "    progress(0.4)\n",
        "    audio_file = f'{audio_file_number}.mp3'\n",
        "    audio_file_number += 1\n",
        "\n",
        "    result = transcribe_audio(audio_file, return_timestamps=True)\n",
        "    progress(0.7)\n",
        "\n",
        "    result = format_whisper_jax_output(result, max_duration=max_duration)\n",
        "    progress(0.9)\n",
        "\n",
        "    with open('audio.json', 'w') as f:\n",
        "        json.dump(result, f)\n"
      ],
      "metadata": {
        "id": "v3HLNmCVXaS8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load data"
      ],
      "metadata": {
        "id": "syFJmVkJPYfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metadata_func(record: dict, metadata: dict) -> dict:\n",
        "\n",
        "    metadata['start'] = record.get('start')\n",
        "    metadata['end'] = record.get('end')\n",
        "    metadata['source'] =  metadata['start'] + '->' + metadata['end']\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    loader = JSONLoader(\n",
        "        file_path='audio.json',\n",
        "        jq_schema='.[]',\n",
        "        content_key='text',\n",
        "        metadata_func=metadata_func\n",
        "    )\n",
        "\n",
        "    data = loader.load()\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "5jjqkPjDPYfN",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:20:38.479022Z",
          "iopub.execute_input": "2024-01-06T07:20:38.479668Z",
          "iopub.status.idle": "2024-01-06T07:20:38.486202Z",
          "shell.execute_reply.started": "2024-01-06T07:20:38.479631Z",
          "shell.execute_reply": "2024-01-06T07:20:38.485227Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create embeddings and vector store"
      ],
      "metadata": {
        "id": "afNgaeFDPYfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embedding_model_kwargs = {'device': device}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name,\n",
        "                                   model_kwargs=embedding_model_kwargs)\n",
        "\n",
        "def create_vectordb(data, k: int):\n",
        "    '''\n",
        "    `k` is the number of retrieved documents\n",
        "    '''\n",
        "\n",
        "    vectordb = Chroma.from_documents(documents=data, embedding=embeddings)\n",
        "    retriever = vectordb.as_retriever(search_type='similarity',\n",
        "                                      search_kwargs={'k': k})\n",
        "\n",
        "    return vectordb, retriever"
      ],
      "metadata": {
        "id": "e67FPeyiPYfN",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:17:32.740092Z",
          "iopub.execute_input": "2024-01-06T07:17:32.740613Z",
          "iopub.status.idle": "2024-01-06T07:17:37.922015Z",
          "shell.execute_reply.started": "2024-01-06T07:17:32.740577Z",
          "shell.execute_reply": "2024-01-06T07:17:37.921014Z"
        },
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load LLM"
      ],
      "metadata": {
        "id": "S9h-t8E9PYfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
        "repo_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
        "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={'max_length': 1024})"
      ],
      "metadata": {
        "id": "pkoefkhBPYfP",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:17:37.923661Z",
          "iopub.execute_input": "2024-01-06T07:17:37.923956Z",
          "iopub.status.idle": "2024-01-06T07:17:38.053088Z",
          "shell.execute_reply.started": "2024-01-06T07:17:37.923930Z",
          "shell.execute_reply": "2024-01-06T07:17:38.052295Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Summarisation\n",
        "We will use the map-reduce method for summarisation. The documents will be first summarised individually (map step). Then their summaries are combined and reduced further to give a single global summary (reduce step)."
      ],
      "metadata": {
        "id": "rdJw1fWT5qsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map\n",
        "map_template = \"\"\"Summarise the following text:\n",
        "{docs}\n",
        "\n",
        "Answer:\"\"\"\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T08:18:21.697259Z",
          "iopub.execute_input": "2024-01-06T08:18:21.698165Z",
          "iopub.status.idle": "2024-01-06T08:18:21.703093Z",
          "shell.execute_reply.started": "2024-01-06T08:18:21.698128Z",
          "shell.execute_reply": "2024-01-06T08:18:21.702157Z"
        },
        "trusted": true,
        "id": "Rm7hrfpr5qsv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce\n",
        "reduce_template = \"\"\"The following is a set of summaries:\n",
        "{docs}\n",
        "\n",
        "Take these and distill it into a final, consolidated summary of the main themes.\n",
        "Answer:\"\"\"\n",
        "\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to llm\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T08:18:22.806311Z",
          "iopub.execute_input": "2024-01-06T08:18:22.806638Z",
          "iopub.status.idle": "2024-01-06T08:18:22.811923Z",
          "shell.execute_reply.started": "2024-01-06T08:18:22.806611Z",
          "shell.execute_reply": "2024-01-06T08:18:22.811032Z"
        },
        "trusted": true,
        "id": "EHc4YJI75qsw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After summarising individual documents, the combined summaries could still exceed the max tokens. In that case, we pass in the summaries in batches and create batched summaries. Once the combined batched summaries are less than the max tokens, we pass them all to the LLM."
      ],
      "metadata": {
        "id": "hl8wyioE5qsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combines and iteravely reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=4000,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T08:18:24.565186Z",
          "iopub.execute_input": "2024-01-06T08:18:24.565519Z",
          "iopub.status.idle": "2024-01-06T08:18:24.570555Z",
          "shell.execute_reply.started": "2024-01-06T08:18:24.565491Z",
          "shell.execute_reply": "2024-01-06T08:18:24.569601Z"
        },
        "trusted": true,
        "id": "8oV56QIk5qsx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we combine our map and reduce chains into one."
      ],
      "metadata": {
        "id": "sHnPQFnZ5qsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        ")\n",
        "\n",
        "def get_summary():\n",
        "    summary = map_reduce_chain.run(data)\n",
        "    return summary"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-06T08:23:58.655895Z",
          "iopub.execute_input": "2024-01-06T08:23:58.656252Z",
          "iopub.status.idle": "2024-01-06T08:23:58.661777Z",
          "shell.execute_reply.started": "2024-01-06T08:23:58.656221Z",
          "shell.execute_reply": "2024-01-06T08:23:58.660851Z"
        },
        "trusted": true,
        "id": "nxY3cWuD5qsy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Q&A\n",
        "### 6.1 Contextualising the question\n",
        "The latest user question may make reference to information in the chat history but we don't want to use the entire history to search for answer in the database because not all of the information is relevant. We want to reformulate the question such that it contains only the relevant information, and can be understood without the chat history."
      ],
      "metadata": {
        "id": "UQOv-Y0AnJSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contextualise_q_prompt = PromptTemplate.from_template(\n",
        "    '''Given a chat history and the latest user question \\\n",
        "    which might reference the chat history, formulate a standalone question \\\n",
        "    which can be understood without the chat history. Do NOT answer the question, \\\n",
        "    just reformulate it if needed and otherwise return it as is.\n",
        "\n",
        "    Chat history: {chat_history}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\n",
        "    '''\n",
        ")\n",
        "\n",
        "contextualise_q_chain = contextualise_q_prompt | llm\n",
        "\n",
        "\n",
        "# Test\n",
        "chat_history = []\n",
        "first_question = 'What is the capital of Australia?'\n",
        "ai_msg = 'Canberra'\n",
        "chat_history.extend([HumanMessage(content=first_question),\n",
        "                     AIMessage(content=ai_msg)])\n",
        "\n",
        "second_question = 'How far is it from Sydney?'\n",
        "answer = contextualise_q_chain.invoke({'question': second_question,\n",
        "                                       'chat_history': chat_history})\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "5Tix4I4iPYfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80894949-cec2-4d7e-e747-f732ff344286",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:17:38.054275Z",
          "iopub.execute_input": "2024-01-06T07:17:38.054652Z",
          "iopub.status.idle": "2024-01-06T07:17:38.726699Z",
          "shell.execute_reply.started": "2024-01-06T07:17:38.054620Z",
          "shell.execute_reply": "2024-01-06T07:17:38.725187Z"
        },
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    What is the distance between Sydney and Canberra?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Standalone question chain\n",
        "Reformulating the question takes time and not all questions need contextualising. To speed up the process, we add a sub-chain which determines whether a question needs contextualising or not. If the question is a standalone question, then we can use the user input directly without modifications."
      ],
      "metadata": {
        "id": "C65d6aO5ZQT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standalone_prompt = PromptTemplate.from_template(\n",
        "    '''Given a chat history and the latest user question, \\\n",
        "    identify whether the question is a standalone question or the question \\\n",
        "    references the chat history. Answer 'yes' if the question is a standalone \\\n",
        "    question, and 'no' if the question references the chat history. Do not \\\n",
        "    answer anything other than 'yes' or 'no'.\n",
        "\n",
        "    Chat history:\n",
        "    {chat_history}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    '''\n",
        ")\n",
        "\n",
        "def format_output(answer: str) -> str:\n",
        "    # All lower case and remove all whitespace\n",
        "    return ''.join(answer.lower().split())\n",
        "\n",
        "standalone_chain = standalone_prompt | llm | format_output\n",
        "\n",
        "\n",
        "# Test\n",
        "answer = standalone_chain.invoke({'question': second_question,\n",
        "                                  'chat_history': chat_history})\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwyWJz7QTulv",
        "outputId": "724f626f-1710-439b-e2ed-fe84a711603f",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:17:38.729806Z",
          "iopub.execute_input": "2024-01-06T07:17:38.730156Z",
          "iopub.status.idle": "2024-01-06T07:17:38.976464Z",
          "shell.execute_reply.started": "2024-01-06T07:17:38.730127Z",
          "shell.execute_reply": "2024-01-06T07:17:38.975580Z"
        },
        "trusted": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Q&A chain\n",
        "Finally, we can build our Q&A chain. The process goes as follows:\n",
        "\n",
        "\n",
        "1.   Check whether the latest user question needs contextualising or not, using the `standalone_chain`.\n",
        "2.   If the question is a standalone question, use it to retrieve documents from the database. Otherwise, reformulate the question using `contextualise_q_chain` to get a contextualised question and use it to retrieve documents from the database.\n",
        "3.   Pass the retrieved documents as `context`, together with the contextualised question to the LLM to receive an answer.\n",
        "\n",
        "\n",
        "We do not want the LLM to use outside knowledge so we tell the LLM that it can only use the information given in `context` to answer the question.\n",
        "\n"
      ],
      "metadata": {
        "id": "tyAN4SocnNm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_prompt = PromptTemplate.from_template(\n",
        "    '''You are an assistant for question-answering tasks. \\\n",
        "    ONLY use the following context to answer the question. \\\n",
        "    Do NOT answer with information that is not contained in \\\n",
        "    the context. If you don't know the answer, just say:\\\n",
        "    \"Sorry, I cannot find the answer to that question in the video.\"\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    '''\n",
        ")\n",
        "\n",
        "\n",
        "def format_docs(docs: list) -> str:\n",
        "    '''\n",
        "    Combine documents\n",
        "    '''\n",
        "    global sources\n",
        "    sources = [doc.metadata['start'] for doc in docs]\n",
        "\n",
        "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "def standalone_question(input_: dict) -> str:\n",
        "    '''\n",
        "    If the question is a not a standalone question, run contextualise_q_chain\n",
        "    '''\n",
        "    if input_['standalone']=='yes':\n",
        "        return contextualise_q_chain\n",
        "    else:\n",
        "        return input_['question']\n",
        "\n",
        "\n",
        "def format_answer(answer: str,\n",
        "                  n_sources: int=1,\n",
        "                  timestamp_interval: datetime.timedelta=datetime.timedelta(minutes=5)) -> str:\n",
        "\n",
        "    if 'cannot find the answer' in answer:\n",
        "        return answer.strip()\n",
        "    else:\n",
        "        timestamps = filter_timestamps(n_sources, timestamp_interval)\n",
        "        answer_with_sources = (answer.strip()\n",
        "        + ' You can find more information at these timestamps: {}.'.format(', '.join(timestamps))\n",
        "        )\n",
        "        return answer_with_sources\n",
        "\n",
        "\n",
        "def filter_timestamps(n_sources: int,\n",
        "                      timestamp_interval: datetime.timedelta=datetime.timedelta(minutes=5)) -> list:\n",
        "    '''Returns a list of timestamps with length `n_sources`.\n",
        "    The timestamps are at least an `timestamp_interval` apart.\n",
        "    This prevents returning a list of timestamps that are too\n",
        "    close together.\n",
        "    '''\n",
        "    sorted_timestamps = sorted(sources)\n",
        "    output = [sorted_timestamps[0]]\n",
        "    i=1\n",
        "    while len(output)<n_sources:\n",
        "        timestamp1 = datetime.datetime.strptime(output[-1], '%H:%M:%S')\n",
        "\n",
        "        try:\n",
        "            timestamp2 = datetime.datetime.strptime(sorted_timestamps[i], '%H:%M:%S')\n",
        "        except IndexError:\n",
        "            break\n",
        "\n",
        "        time_diff = timestamp2 - timestamp1\n",
        "\n",
        "        if time_diff>timestamp_interval:\n",
        "            output.append(str(timestamp2.time()))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def setup_rag(url):\n",
        "    '''Given a YouTube url, set up the vector database and the RAG chain.\n",
        "    '''\n",
        "\n",
        "    yt_audio_to_text(url)\n",
        "\n",
        "    global data\n",
        "    data = load_data()\n",
        "\n",
        "    global retriever\n",
        "    _, retriever = create_vectordb(data, k)\n",
        "\n",
        "    global rag_chain\n",
        "    rag_chain = (\n",
        "        RunnablePassthrough.assign(standalone=standalone_chain)\n",
        "        | {'question':standalone_question,\n",
        "           'context':standalone_question|retriever|format_docs\n",
        "          }\n",
        "        | qa_prompt\n",
        "        | llm\n",
        "    )\n",
        "\n",
        "    return url\n",
        "\n",
        "\n",
        "\n",
        "def get_answer(question: str) -> str:\n",
        "\n",
        "    global chat_history\n",
        "\n",
        "    ai_msg = rag_chain.invoke({'question': question,\n",
        "                               'chat_history': chat_history\n",
        "                              })\n",
        "\n",
        "    answer = format_answer(ai_msg, n_sources, timestamp_interval)\n",
        "\n",
        "    chat_history.extend([HumanMessage(content=question),\n",
        "                         AIMessage(content=answer)])\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "-t8Rw-gtPYfR",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:22:54.000019Z",
          "iopub.execute_input": "2024-01-06T07:22:54.000360Z",
          "iopub.status.idle": "2024-01-06T07:22:54.015260Z",
          "shell.execute_reply.started": "2024-01-06T07:22:54.000333Z",
          "shell.execute_reply": "2024-01-06T07:22:54.014300Z"
        },
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Test our chatbot"
      ],
      "metadata": {
        "id": "osx_GOy6zt9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot settings\n",
        "n_sources = 3 # Number of sources provided in the answer\n",
        "k = 5 # Number of documents returned by the retriever\n",
        "timestamp_interval = datetime.timedelta(minutes=2)\n",
        "chat_history = []\n",
        "url = 'https://www.youtube.com/watch?v=4Bdc55j80l8'\n",
        "setup_rag(url)"
      ],
      "metadata": {
        "id": "Hat_DXPoPYfR",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:20:46.545290Z",
          "iopub.execute_input": "2024-01-06T07:20:46.546176Z",
          "iopub.status.idle": "2024-01-06T07:21:20.075152Z",
          "shell.execute_reply.started": "2024-01-06T07:20:46.546140Z",
          "shell.execute_reply": "2024-01-06T07:21:20.074156Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.4.1 Test 1: A question that cannot be answered by the video alone."
      ],
      "metadata": {
        "id": "NDyc7EeVzzjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is the capital city of Australia?'\n",
        "answer = get_answer(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "N1fTE8vfPYfR",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:22:59.109574Z",
          "iopub.execute_input": "2024-01-06T07:22:59.110352Z",
          "iopub.status.idle": "2024-01-06T07:22:59.307813Z",
          "shell.execute_reply.started": "2024-01-06T07:22:59.110319Z",
          "shell.execute_reply": "2024-01-06T07:22:59.306949Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1b186d-cedb-47c2-c071-2e34061ea8d6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I cannot find the answer to that question in the video.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.4.2 Test 2: A question that can be answered by the video."
      ],
      "metadata": {
        "id": "ckevA0uW0Qsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'In what ways did transformers improve upon RNN?'\n",
        "answer = get_answer(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "tc7_m5u5PYfS",
        "execution": {
          "iopub.status.busy": "2024-01-06T07:23:07.727373Z",
          "iopub.execute_input": "2024-01-06T07:23:07.728141Z",
          "iopub.status.idle": "2024-01-06T07:23:10.210184Z",
          "shell.execute_reply.started": "2024-01-06T07:23:07.728108Z",
          "shell.execute_reply": "2024-01-06T07:23:10.209258Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f29584d-94d4-406f-97b1-99008d2d0cd6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers improved upon RNN in several ways. Firstly, they leverage the power of the attention mechanism, which allows them to have an infinite window to reference from, while RNNs suffer from short-term memory and can only reference a limited window. This makes transformers better at encoding and generating longer sequences. Secondly, transformers can stack multiple layers, each layer taking in inputs from the encoder and the layers before it, allowing them to learn to extract and focus You can find more information at these timestamps: 00:00:00, 00:02:15, 00:13:51.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del rag_chain, retriever, data\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "hc90erEO1oyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Web app"
      ],
      "metadata": {
        "id": "AFHOyf8HPYfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot settings\n",
        "n_sources = 3 # Number of sources provided in the answer\n",
        "k = 5 # Number of documents returned by the retriever\n",
        "timestamp_interval = datetime.timedelta(minutes=2)\n",
        "default_youtube_url = 'https://www.youtube.com/watch?v=4Bdc55j80l8'"
      ],
      "metadata": {
        "id": "Eef-j5rtktla",
        "execution": {
          "iopub.status.busy": "2024-01-04T06:43:17.653395Z",
          "iopub.execute_input": "2024-01-04T06:43:17.654282Z",
          "iopub.status.idle": "2024-01-04T06:43:17.658688Z",
          "shell.execute_reply.started": "2024-01-04T06:43:17.654245Z",
          "shell.execute_reply": "2024-01-04T06:43:17.657714Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greet():\n",
        "    summary = get_summary()\n",
        "    global gradio_chat_history\n",
        "    summary_message = f'Here is a summary of the video \"{video_title}\":'\n",
        "    gradio_chat_history.append((None, summary_message))\n",
        "    gradio_chat_history.append((None, summary))\n",
        "    greeting_message = f'You can ask me anything about the video. I will do my best to answer!'\n",
        "    gradio_chat_history.append((None, greeting_message))\n",
        "    return gradio_chat_history\n",
        "\n",
        "def question(user_message):\n",
        "    global gradio_chat_history\n",
        "    gradio_chat_history.append((user_message, None))\n",
        "    return gradio_chat_history\n",
        "\n",
        "def respond():\n",
        "    global gradio_chat_history\n",
        "    ai_message = get_answer(gradio_chat_history[-1][0])\n",
        "    gradio_chat_history.append((None, ai_message))\n",
        "    return '', gradio_chat_history\n",
        "\n",
        "def clear_chat_history():\n",
        "    global chat_history\n",
        "    global gradio_chat_history\n",
        "    chat_history = []\n",
        "    gradio_chat_history = []"
      ],
      "metadata": {
        "id": "0G-gjmC7PYfS",
        "execution": {
          "iopub.status.busy": "2024-01-04T06:43:18.070537Z",
          "iopub.execute_input": "2024-01-04T06:43:18.071376Z",
          "iopub.status.idle": "2024-01-04T06:43:18.076927Z",
          "shell.execute_reply.started": "2024-01-04T06:43:18.071341Z",
          "shell.execute_reply": "2024-01-04T06:43:18.075991Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "gradio_chat_history = []\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    # Structure\n",
        "    with gr.Row():\n",
        "        url_input = gr.Textbox(value=default_youtube_url,\n",
        "                               label='YouTube URL',\n",
        "                               scale=5)\n",
        "        button = gr.Button(value='Go', scale=1)\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    user_message = gr.Textbox(label='Ask a question:')\n",
        "    clear = gr.ClearButton([user_message, chatbot])\n",
        "\n",
        "\n",
        "    # Actions\n",
        "    button.click(setup_rag,\n",
        "                 inputs=[url_input],\n",
        "                 outputs=[url_input],\n",
        "                 trigger_mode='once').then(greet,\n",
        "                                           inputs=[],\n",
        "                                           outputs=[chatbot])\n",
        "\n",
        "    user_message.submit(question,\n",
        "                        inputs=[user_message],\n",
        "                        outputs=[chatbot]).then(respond,\n",
        "                                                inputs=[],\n",
        "                                                outputs=[user_message, chatbot])\n",
        "\n",
        "    clear.click(clear_chat_history)\n",
        "\n",
        "\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "UNvljS1mlBiZ",
        "execution": {
          "iopub.status.busy": "2024-01-04T06:43:18.784008Z",
          "iopub.execute_input": "2024-01-04T06:43:18.784845Z",
          "iopub.status.idle": "2024-01-04T06:43:39.478991Z",
          "shell.execute_reply.started": "2024-01-04T06:43:18.784810Z",
          "shell.execute_reply": "2024-01-04T06:43:39.478127Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yHBJBzL5RZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}