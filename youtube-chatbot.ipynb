{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ww-6/youtube-chatbot/blob/main/youtube-chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YouTube Chatbot"
      ],
      "metadata": {
        "id": "JSe0VTTc6eWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes langchain chromadb jq \\\n",
        "sentence-transformers gradio yt_dlp\n",
        "\n",
        "!pip install -q git+https://github.com/m-bain/whisperx.git"
      ],
      "metadata": {
        "id": "2G5l5uag6iCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTsK_hF3PYfL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import GenerationConfig\n",
        "from transformers import pipeline\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import JSONLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "import whisperx\n",
        "import yt_dlp\n",
        "import json\n",
        "import gc\n",
        "import gradio as gr\n",
        "import datetime\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGnYb4fhPYfL"
      },
      "source": [
        "## 1. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDbCV-HNPYfM"
      },
      "outputs": [],
      "source": [
        "audio_file_number = 1\n",
        "def yt_audio_to_text(url,\n",
        "                     device = \"cuda\",\n",
        "                     batch_size = 8,\n",
        "                     compute_type = \"float32\"\n",
        "                    ):\n",
        "    global audio_file_number\n",
        "    global progress\n",
        "    progress = gr.Progress()\n",
        "    progress(0.1)\n",
        "\n",
        "    with yt_dlp.YoutubeDL({'extract_audio': True,\n",
        "                           'format': 'bestaudio',\n",
        "                           'outtmpl': f'{audio_file_number}.mp3'}) as video:\n",
        "\n",
        "        info_dict = video.extract_info(url, download=False)\n",
        "        global video_title\n",
        "        video_title = info_dict['title']\n",
        "        video.download(url)\n",
        "\n",
        "    progress(0.3)\n",
        "    audio_file = f'{audio_file_number}.mp3'\n",
        "    audio_file_number += 1\n",
        "\n",
        "\n",
        "    model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "    progress(0.6)\n",
        "\n",
        "    audio = whisperx.load_audio(audio_file)\n",
        "\n",
        "    result = model.transcribe(audio, batch_size=batch_size)\n",
        "    progress(0.9)\n",
        "\n",
        "    with open('audio.json', 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    del model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syFJmVkJPYfM"
      },
      "source": [
        "## 2. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jjqkPjDPYfN"
      },
      "outputs": [],
      "source": [
        "def metadata_func(record: dict, metadata: dict) -> dict:\n",
        "\n",
        "    metadata['start'] = str(datetime.timedelta(seconds=round(record.get(\"start\"))))\n",
        "    metadata['end'] = str(datetime.timedelta(seconds=round(record.get(\"end\"))))\n",
        "    metadata['source'] =  metadata['start'] + '->' + metadata['end']\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    loader = JSONLoader(\n",
        "        file_path='audio.json',\n",
        "        jq_schema='.segments[]',\n",
        "        content_key='text',\n",
        "        metadata_func=metadata_func\n",
        "    )\n",
        "\n",
        "    global data\n",
        "    data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afNgaeFDPYfN"
      },
      "source": [
        "## 3. Create embeddings and vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e67FPeyiPYfN"
      },
      "outputs": [],
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name,\n",
        "                                   model_kwargs=model_kwargs)\n",
        "\n",
        "def create_vectordb(k):\n",
        "    global data\n",
        "    global vectordb\n",
        "    global retriever\n",
        "    vectordb = Chroma.from_documents(documents=data, embedding=embeddings)\n",
        "    retriever = vectordb.as_retriever(search_type=\"similarity\",\n",
        "                                      search_kwargs={\"k\": k})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9h-t8E9PYfO"
      },
      "source": [
        "## 4. Load LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkoefkhBPYfP"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"max_length\": 1024})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C65d6aO5ZQT8"
      },
      "source": [
        "## 5. Standalone question chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpsy_DGxPYfQ"
      },
      "outputs": [],
      "source": [
        "standalone_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Given a chat history and the latest user question, \\\n",
        "    identify whether the question is a standalone question or the question \\\n",
        "    references the chat history. Answer 'yes' if the question is a standalone \\\n",
        "    question, and 'no' if the question references the chat history. Do not \\\n",
        "    answer anything other than 'yes' or 'no'.\n",
        "\n",
        "    Chat history:\n",
        "    {chat_history}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "def format_output(answer: str) -> str:\n",
        "    # All lower case and remove all whitespace\n",
        "    return ''.join(answer.lower().split())\n",
        "\n",
        "standalone_chain = standalone_prompt | llm | format_output\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "template = \"\"\"\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = prompt | llm\n",
        "\n",
        "\n",
        "# Test\n",
        "question = \"What is the capital of Australia?\"\n",
        "ai_msg = chain.invoke({'question':question})\n",
        "chat_history.extend([HumanMessage(content=question),\n",
        "                     AIMessage(content=ai_msg)])\n",
        "\n",
        "second_question = \"What is the population?\"\n",
        "answer = standalone_chain.invoke({'question': second_question,\n",
        "                                  'chat_history': chat_history})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQOv-Y0AnJSq"
      },
      "source": [
        "## 6. Condense question chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tix4I4iPYfQ"
      },
      "outputs": [],
      "source": [
        "condense_q_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Given a chat history and the latest user question \\\n",
        "    which might reference the chat history, formulate a standalone question \\\n",
        "    which can be understood without the chat history. Do NOT answer the question, \\\n",
        "    just reformulate it if needed and otherwise return it as is.\n",
        "\n",
        "    Chat history: {chat_history}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "condense_q_chain = condense_q_prompt | llm\n",
        "\n",
        "\n",
        "# Test\n",
        "answer = condense_q_chain.invoke({'question': second_question,\n",
        "                                  'chat_history': chat_history})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyAN4SocnNm9"
      },
      "source": [
        "## 7. Q&A chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t8Rw-gtPYfR"
      },
      "outputs": [],
      "source": [
        "qa_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant for question-answering tasks. \\\n",
        "    Only use the following context to answer the question. \\\n",
        "    Do not answer with information that is not contained in \\\n",
        "    the context. If you don't know the answer, just say the \\\n",
        "    following in exact words: {no_answer_msg}.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "def format_docs(docs: list) -> str:\n",
        "    '''\n",
        "    Combine documents\n",
        "    '''\n",
        "    global sources\n",
        "    sources = [doc.metadata['start'] for doc in docs]\n",
        "\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "def standalone_question(input_: dict) -> str:\n",
        "    '''\n",
        "    If the question is a not a standalone question, run condense_q_chain\n",
        "    '''\n",
        "    if input_['standalone']=='yes':\n",
        "        return condense_q_chain\n",
        "    else:\n",
        "        return input_['question']\n",
        "\n",
        "\n",
        "def format_answer(answer: str,\n",
        "                  n_sources: int=1,\n",
        "                  timestamp_interval: datetime.timedelta=datetime.timedelta(minutes=5)) -> str:\n",
        "\n",
        "    no_whitespace_answer = ' '.join(answer.split())\n",
        "\n",
        "    if no_whitespace_answer == no_answer_msg:\n",
        "        return no_answer_msg\n",
        "    else:\n",
        "        timestamps = filter_timestamps(n_sources, timestamp_interval)\n",
        "        answer_with_sources = (answer.strip()\n",
        "        + ' You can find more information at these timestamps: {}.'.format(', '.join(timestamps))\n",
        "        )\n",
        "        return answer_with_sources\n",
        "\n",
        "\n",
        "def filter_timestamps(n_sources: int,\n",
        "                      timestamp_interval: datetime.timedelta=datetime.timedelta(minutes=5)) -> list:\n",
        "    '''Returns a list of timestamps with length `n_sources`.\n",
        "    The timestamps are at least an `timestamp_interval` apart.\n",
        "    '''\n",
        "    sorted_timestamps = sorted(sources)\n",
        "    output = [sorted_timestamps[0]]\n",
        "    i=1\n",
        "    while len(output)<n_sources:\n",
        "        timestamp1 = datetime.datetime.strptime(output[-1], '%H:%M:%S')\n",
        "\n",
        "        try:\n",
        "            timestamp2 = datetime.datetime.strptime(sorted_timestamps[i], '%H:%M:%S')\n",
        "        except IndexError:\n",
        "            break\n",
        "\n",
        "        time_diff = timestamp2 - timestamp1\n",
        "\n",
        "        if time_diff>timestamp_interval:\n",
        "            output.append(str(timestamp2.time()))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def setup_rag(url):\n",
        "    global retriever\n",
        "    global rag_chain\n",
        "\n",
        "    yt_audio_to_text(url)\n",
        "    load_data()\n",
        "    create_vectordb(k)\n",
        "\n",
        "    rag_chain = (\n",
        "        RunnablePassthrough.assign(standalone=standalone_chain)\n",
        "        | {'question':standalone_question,\n",
        "           'context':standalone_question|retriever|format_docs,\n",
        "           'no_answer_msg':lambda input_: input_['no_answer_msg']\n",
        "          }\n",
        "        | qa_prompt\n",
        "        | llm\n",
        "    )\n",
        "\n",
        "    return url\n",
        "\n",
        "\n",
        "\n",
        "def get_answer(question: str) -> str:\n",
        "    global chat_history\n",
        "    global rag_chain\n",
        "    ai_msg = rag_chain.invoke({\"question\": question,\n",
        "                               \"chat_history\": chat_history,\n",
        "                               \"no_answer_msg\": no_answer_msg\n",
        "                              })\n",
        "\n",
        "    answer = format_answer(ai_msg, n_sources, timestamp_interval)\n",
        "\n",
        "    chat_history.extend([HumanMessage(content=question), AIMessage(content=answer)])\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Test our chatbot"
      ],
      "metadata": {
        "id": "osx_GOy6zt9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hat_DXPoPYfR"
      },
      "outputs": [],
      "source": [
        "no_answer_msg = \"Sorry, I cannot find the answer to that question in the video.\"\n",
        "n_sources = 3 # Number of sources provided in the answer\n",
        "k = 10 # Number of documents returned by the retriever\n",
        "timestamp_interval = datetime.timedelta(minutes=5)\n",
        "chat_history = []\n",
        "url = 'https://www.youtube.com/watch?v=SZorAJ4I-sA'\n",
        "setup_rag(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Test 1: A question that cannot be answered by the video alone."
      ],
      "metadata": {
        "id": "NDyc7EeVzzjj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1fTE8vfPYfR"
      },
      "outputs": [],
      "source": [
        "question = \"Where is Canberra?\"\n",
        "answer = get_answer(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Test 2: A question that can be answered by the video."
      ],
      "metadata": {
        "id": "ckevA0uW0Qsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc7_m5u5PYfS"
      },
      "outputs": [],
      "source": [
        "question = \"What are transformers good at?\"\n",
        "answer = get_answer(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del rag_chain, retriever, vectordb, data\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "hc90erEO1oyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFHOyf8HPYfS"
      },
      "source": [
        "## 9. Web app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eef-j5rtktla"
      },
      "outputs": [],
      "source": [
        "# Chatbot settings\n",
        "no_answer_msg = \"Sorry, I cannot find the answer to that question in the video.\"\n",
        "n_sources = 3 # Number of sources provided in the answer\n",
        "k = 10 # Number of documents returned by the retriever\n",
        "timestamp_interval = datetime.timedelta(minutes=3)\n",
        "default_youtube_url = 'https://www.youtube.com/watch?v=SZorAJ4I-sA'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G-gjmC7PYfS"
      },
      "outputs": [],
      "source": [
        "def greet():\n",
        "    global gradio_chat_history\n",
        "    greeting_message = f'You can ask me anything about the video \"{video_title}\". I will do my best to answer!'\n",
        "    gradio_chat_history.append((None, greeting_message))\n",
        "    return gradio_chat_history\n",
        "\n",
        "def respond(message):\n",
        "    global gradio_chat_history\n",
        "    ai_message = get_answer(message)\n",
        "    gradio_chat_history.append((message, ai_message))\n",
        "    return \"\", gradio_chat_history\n",
        "\n",
        "def clear_chat_history():\n",
        "    global chat_history\n",
        "    global gradio_chat_history\n",
        "    chat_history=[]\n",
        "    gradio_chat_history=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNvljS1mlBiZ"
      },
      "outputs": [],
      "source": [
        "chat_history = []\n",
        "gradio_chat_history = []\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    # Structure\n",
        "    with gr.Row():\n",
        "        url_input = gr.Textbox(value=default_youtube_url,\n",
        "                               label='YouTube URL',\n",
        "                               scale=5)\n",
        "        button = gr.Button(value='Go', scale=1)\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    user_message = gr.Textbox(label='Ask a question:')\n",
        "    clear = gr.ClearButton([user_message, chatbot])\n",
        "\n",
        "\n",
        "    # Actions\n",
        "    button.click(setup_rag,\n",
        "                 inputs=[url_input],\n",
        "                 outputs=[url_input],\n",
        "                 trigger_mode='once').then(greet,\n",
        "                                           inputs=[],\n",
        "                                           outputs=[chatbot])\n",
        "\n",
        "    user_message.submit(respond,\n",
        "                        inputs=[user_message],\n",
        "                        outputs=[user_message, chatbot])\n",
        "\n",
        "    clear.click(clear_chat_history)\n",
        "\n",
        "\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yHBJBzL5RZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}