{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ww-6/youtube-chatbot/blob/main/youtube-chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSe0VTTc6eWO"
      },
      "source": [
        "# YouTube Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G5l5uag6iCN"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes langchain \\\n",
        "langchain-community chromadb jq sentence-transformers gradio yt_dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTsK_hF3PYfL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain.chains import LLMChain, StuffDocumentsChain, MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain.memory.buffer_window import ConversationBufferWindowMemory\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "import yt_dlp\n",
        "import json\n",
        "import gc\n",
        "import gradio as gr\n",
        "from gradio_client import Client\n",
        "import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGnYb4fhPYfL"
      },
      "source": [
        "## 1. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDbCV-HNPYfM",
        "outputId": "ae32b7cb-2ec1-463b-bb7c-521166cedd13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded as API: https://sanchit-gandhi-whisper-jax.hf.space/ âœ”\n"
          ]
        }
      ],
      "source": [
        "whisper_jax_api = 'https://sanchit-gandhi-whisper-jax.hf.space/'\n",
        "whisper_jax = Client(whisper_jax_api)\n",
        "\n",
        "def transcribe_audio(audio_path,\n",
        "                     task='transcribe',\n",
        "                     return_timestamps=True) -> str:\n",
        "\n",
        "    text, runtime = whisper_jax.predict(\n",
        "        audio_path,\n",
        "        task,\n",
        "        return_timestamps,\n",
        "        api_name='/predict_1',\n",
        "    )\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izkQmZqkXWO_"
      },
      "outputs": [],
      "source": [
        "def format_whisper_jax_output(whisper_jax_output: str,\n",
        "                              max_duration: int = 60) -> list[dict]:\n",
        "\n",
        "    \"\"\"Returns a list of dict with keys 'start', 'end', 'text'\n",
        "    The segments from whisper jax output are merged to form paragraphs.\n",
        "\n",
        "    `max_duration` controls how many seconds of the audio's transcripts are merged\n",
        "\n",
        "    For example, if `max_duration`=60, in the final output, each segment is roughly\n",
        "    60 seconds.\n",
        "    \"\"\"\n",
        "\n",
        "    final_output = []\n",
        "    max_duration = datetime.timedelta(seconds=max_duration)\n",
        "    segments = whisper_jax_output.split('\\n')\n",
        "    current_start = datetime.datetime.strptime('00:00', '%M:%S')\n",
        "    current_text = ''\n",
        "\n",
        "    for i, seg in enumerate(segments):\n",
        "\n",
        "        text = seg.split(']')[-1].strip()\n",
        "\n",
        "        # Sometimes whisper jax returns None for timestamp\n",
        "        try:\n",
        "            end = datetime.datetime.strptime(seg[14:19], '%M:%S')\n",
        "        except ValueError:\n",
        "            end = current_start + max_duration\n",
        "\n",
        "        if (end-current_start >= max_duration) or (i == len(segments)-1):\n",
        "            # If we have exceeded max duration or at the last segment,\n",
        "            # stop merging and append to final_output.\n",
        "\n",
        "            current_text += text\n",
        "            final_output.append({\n",
        "                'start': current_start.strftime('%H:%M:%S'),\n",
        "                'end': end.strftime('%H:%M:%S'),\n",
        "                'text': current_text\n",
        "            })\n",
        "\n",
        "            # Update current start and text\n",
        "            current_start = end\n",
        "            current_text = ''\n",
        "\n",
        "        else:\n",
        "            # If we have not exceeded max duration, keep merging.\n",
        "            current_text += text\n",
        "\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3HLNmCVXaS8"
      },
      "outputs": [],
      "source": [
        "audio_file_number = 1\n",
        "def yt_audio_to_text(url: str,\n",
        "                     max_duration: int = 60\n",
        "                    ):\n",
        "\n",
        "    global audio_file_number\n",
        "    global progress\n",
        "    progress = gr.Progress()\n",
        "    progress(0.1)\n",
        "\n",
        "    with yt_dlp.YoutubeDL({'extract_audio': True,\n",
        "                           'format': 'bestaudio',\n",
        "                           'outtmpl': f'{audio_file_number}.mp3'\n",
        "                          }) as video:\n",
        "\n",
        "        info_dict = video.extract_info(url, download=False)\n",
        "        global video_title\n",
        "        video_title = info_dict['title']\n",
        "        video.download(url)\n",
        "\n",
        "    progress(0.4)\n",
        "    audio_file = f'{audio_file_number}.mp3'\n",
        "    audio_file_number += 1\n",
        "\n",
        "    result = transcribe_audio(audio_file, return_timestamps=True)\n",
        "    progress(0.7)\n",
        "\n",
        "    result = format_whisper_jax_output(result, max_duration=max_duration)\n",
        "    progress(0.9)\n",
        "\n",
        "    with open('audio.json', 'w') as f:\n",
        "        json.dump(result, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syFJmVkJPYfM"
      },
      "source": [
        "## 2. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jjqkPjDPYfN"
      },
      "outputs": [],
      "source": [
        "def metadata_func(record: dict, metadata: dict) -> dict:\n",
        "\n",
        "    metadata['start'] = record.get('start')\n",
        "    metadata['end'] = record.get('end')\n",
        "    metadata['source'] =  metadata['start'] + ' -> ' + metadata['end']\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    loader = JSONLoader(\n",
        "        file_path='audio.json',\n",
        "        jq_schema='.[]',\n",
        "        content_key='text',\n",
        "        metadata_func=metadata_func\n",
        "    )\n",
        "\n",
        "    data = loader.load()\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afNgaeFDPYfN"
      },
      "source": [
        "## 3. Create embeddings and vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e67FPeyiPYfN"
      },
      "outputs": [],
      "source": [
        "embedding_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embedding_model_kwargs = {'device': device}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name,\n",
        "                                   model_kwargs=embedding_model_kwargs)\n",
        "\n",
        "def create_vectordb(data, k: int):\n",
        "    \"\"\"Returns a vector database, and its retriever\n",
        "    `k` is the number of retrieved documents\n",
        "    \"\"\"\n",
        "\n",
        "    vectordb = Chroma.from_documents(documents=data, embedding=embeddings)\n",
        "    retriever = vectordb.as_retriever(search_type='similarity',\n",
        "                                      search_kwargs={'k': k})\n",
        "\n",
        "    return vectordb, retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9h-t8E9PYfO"
      },
      "source": [
        "## 4. Load LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkoefkhBPYfP"
      },
      "outputs": [],
      "source": [
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
        "repo_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
        "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={'max_new_tokens': 1000})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdJw1fWT5qsv"
      },
      "source": [
        "## 5. Summarisation\n",
        "We will use the map-reduce method for summarisation. The documents will be first summarised individually (map step). Then their summaries are combined and reduced further to give a single global summary (reduce step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm7hrfpr5qsv"
      },
      "outputs": [],
      "source": [
        "# Map\n",
        "map_template = \"\"\"Summarise the following text:\n",
        "{docs}\n",
        "\n",
        "Answer:\"\"\"\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHc4YJI75qsw"
      },
      "outputs": [],
      "source": [
        "# Reduce\n",
        "reduce_template = \"\"\"The following is a set of summaries:\n",
        "{docs}\n",
        "\n",
        "Take these and distill it into a final, consolidated summary of the main themes \\\n",
        "in 150 words or less.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to llm\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl8wyioE5qsw"
      },
      "source": [
        "After summarising individual documents, the combined summaries could still exceed the max tokens. In that case, we pass in the summaries in batches and create batched summaries. Once all the batched summaries combined are less than the max tokens, we pass them all to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oV56QIk5qsx"
      },
      "outputs": [],
      "source": [
        "# Combines and iteravely reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=4000\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHnPQFnZ5qsy"
      },
      "source": [
        "Finally, we combine our map and reduce chains into one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxY3cWuD5qsy"
      },
      "outputs": [],
      "source": [
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False\n",
        ")\n",
        "\n",
        "def get_summary(documents) -> str:\n",
        "    summary = map_reduce_chain.invoke(documents, return_only_outputs=True)\n",
        "    return summary['output_text'].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQOv-Y0AnJSq"
      },
      "source": [
        "## 6. Q&A\n",
        "### 6.1 Contextualising the question\n",
        "The latest user question may make reference to information in the chat history but we don't want to use the entire history to search for answer in the database because not all of the information is relevant. We want to reformulate the question such that it contains only the relevant information, and can be understood without the chat history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tix4I4iPYfQ"
      },
      "outputs": [],
      "source": [
        "contextualise_q_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Given a chat history and the latest user question \\\n",
        "    which might reference the chat history, formulate a standalone question \\\n",
        "    that can be understood without the chat history. Do NOT answer the question, \\\n",
        "    just reformulate it if needed and otherwise return it as is.\n",
        "\n",
        "    Chat history: {chat_history}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "contextualise_q_chain = contextualise_q_prompt | llm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our `contextualise_q_chain` and see if it does what we want. Suppose we have the following conversation:\n",
        "    \n",
        "> Human: What is the capital city of Australia?\n",
        "\n",
        "> AI: Canberra.\n",
        "\n",
        "> Human: How far is it from Sydney?\n",
        "\n",
        "We know that \"it\" here refers to Canberra. Let's see if the LLM can rephrase the question to include Canberra in the question."
      ],
      "metadata": {
        "id": "ZgFkxlT_-36Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "chat_history = []\n",
        "first_question = 'What is the capital city of Australia?'\n",
        "ai_msg = 'Canberra'\n",
        "chat_history.extend([HumanMessage(content=first_question),\n",
        "                     AIMessage(content=ai_msg)])\n",
        "\n",
        "second_question = 'How far is it from Sydney?'\n",
        "answer = contextualise_q_chain.invoke({'question': second_question,\n",
        "                                       'chat_history': chat_history})\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "849hn8TA-ytz",
        "outputId": "390fd787-7eaf-43e1-a5aa-e1c1d088a542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    What is the distance between Sydney and Canberra?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C65d6aO5ZQT8"
      },
      "source": [
        "### 6.2 Standalone question chain\n",
        "Reformulating the question takes time and not all questions need contextualising. To speed up the process, we add a sub-chain which determines whether a question needs contextualising or not. If the question is a standalone question, then we can use the user input directly without modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwyWJz7QTulv"
      },
      "outputs": [],
      "source": [
        "standalone_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Given a chat history and the latest user question, \\\n",
        "    identify whether the question is a standalone question or the question \\\n",
        "    references the chat history. Answer 'yes' if the question is a standalone \\\n",
        "    question, and 'no' if the question references the chat history. Do not \\\n",
        "    answer anything other than 'yes' or 'no'.\n",
        "\n",
        "    Chat history:\n",
        "    {chat_history}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "def format_output(answer: str) -> str:\n",
        "    # All lower case and remove all whitespace\n",
        "    return ''.join(answer.lower().split())\n",
        "\n",
        "standalone_chain = standalone_prompt | llm | format_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the `standalone_chain` using the same example as before. Recall that the question \"How far is it from Sydney?\" refers to the answer given by the AI so it is not a standalone question. Let's see if the LLM can pick up on that."
      ],
      "metadata": {
        "id": "VpzjL-qwAntr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "answer = standalone_chain.invoke({'question': second_question,\n",
        "                                  'chat_history': chat_history})\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ISIXJ6TAk33",
        "outputId": "f162bbed-011a-4bbd-9848-12cc405c3ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyAN4SocnNm9"
      },
      "source": [
        "### 6.3 Q&A chain\n",
        "Finally, we can build our Q&A chain. The process goes as follows:\n",
        "\n",
        "\n",
        "1.   Check whether the latest user question needs contextualising or not, using the `standalone_chain`.\n",
        "2.   If the question is a standalone question, use it to retrieve documents from the database. Otherwise, reformulate the question using `contextualise_q_chain` to get a contextualised question and use it to retrieve documents from the database.\n",
        "3.   Pass the retrieved documents as `context`, together with the contextualised question to the LLM to receive an answer.\n",
        "\n",
        "\n",
        "We do not want the LLM to use outside knowledge so we tell the LLM that it can only use the information given in `context` to answer the question.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t8Rw-gtPYfR"
      },
      "outputs": [],
      "source": [
        "qa_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant for question-answering tasks. \\\n",
        "    ONLY use the following context to answer the question. \\\n",
        "    Do NOT answer with information that is not contained in \\\n",
        "    the context. If you don't know the answer, just say:\\\n",
        "    \"Sorry, I cannot find the answer to that question in the video.\"\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class YouTubeChatbot:\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_sources: int,\n",
        "                 k: int,\n",
        "                 timestamp_interval: datetime.timedelta,\n",
        "                 memory: int,\n",
        "                ):\n",
        "        self.n_sources = n_sources\n",
        "        self.k = k\n",
        "        self.timestamp_interval = timestamp_interval\n",
        "        self.chat_history = ConversationBufferWindowMemory(k=memory)\n",
        "\n",
        "\n",
        "    def format_docs(self, docs: list) -> str:\n",
        "        \"\"\"Combine documents\n",
        "        \"\"\"\n",
        "\n",
        "        self.sources = [doc.metadata['start'] for doc in docs]\n",
        "\n",
        "        return '\\n\\n'.join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "\n",
        "    def standalone_question(self, input_: dict) -> str:\n",
        "        \"\"\"If the question is a not a standalone question,\n",
        "        run contextualise_q_chain.\n",
        "        \"\"\"\n",
        "        if input_['standalone']=='yes':\n",
        "            return contextualise_q_chain\n",
        "        else:\n",
        "            return input_['question']\n",
        "\n",
        "\n",
        "    def format_answer(self, answer: str) -> str:\n",
        "\n",
        "        if 'cannot find the answer' in answer:\n",
        "            return answer.strip()\n",
        "        else:\n",
        "            timestamps = self.filter_timestamps()\n",
        "            answer_with_sources = (\n",
        "                answer.strip()\n",
        "                + ' You can find more information '\\\n",
        "                'at these timestamps: {}.'.format(', '.join(timestamps))\n",
        "                )\n",
        "            return answer_with_sources\n",
        "\n",
        "\n",
        "    def filter_timestamps(self) -> list[str]:\n",
        "        \"\"\"Returns a list of timestamps with length `n_sources`.\n",
        "        The timestamps are at least an `timestamp_interval` apart.\n",
        "        This prevents returning a list of timestamps that are too\n",
        "        close together.\n",
        "        \"\"\"\n",
        "\n",
        "        sorted_timestamps = sorted(self.sources)\n",
        "        filtered_timestamps = [sorted_timestamps[0]]\n",
        "        i=1\n",
        "        while len(filtered_timestamps) < self.n_sources:\n",
        "            timestamp1 = datetime.datetime.strptime(filtered_timestamps[-1],\n",
        "                                                    '%H:%M:%S')\n",
        "\n",
        "            try:\n",
        "                timestamp2 = datetime.datetime.strptime(sorted_timestamps[i],\n",
        "                                                        '%H:%M:%S')\n",
        "            except IndexError:\n",
        "                break\n",
        "\n",
        "            time_diff = timestamp2 - timestamp1\n",
        "\n",
        "            if time_diff>=self.timestamp_interval:\n",
        "                filtered_timestamps.append(str(timestamp2.time()))\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        return filtered_timestamps\n",
        "\n",
        "\n",
        "    def setup_chatbot(self, url: str) -> str:\n",
        "        \"\"\"Given a YouTube url, set up the chatbot.\n",
        "        \"\"\"\n",
        "\n",
        "        yt_audio_to_text(url)\n",
        "\n",
        "        self.data = load_data()\n",
        "\n",
        "        _, self.retriever = create_vectordb(self.data, self.k)\n",
        "\n",
        "\n",
        "        self.qa_chain = (\n",
        "            RunnablePassthrough.assign(standalone=standalone_chain)\n",
        "            | {'question':self.standalone_question,\n",
        "               'context':self.standalone_question|self.retriever|self.format_docs}\n",
        "            | qa_prompt\n",
        "            | llm)\n",
        "\n",
        "        return url\n",
        "\n",
        "\n",
        "\n",
        "    def get_answer(self, question: str) -> str:\n",
        "\n",
        "        try:\n",
        "            ai_msg = self.qa_chain.invoke({'question': question,\n",
        "                                           'chat_history': self.chat_history})\n",
        "        except AttributeError:\n",
        "            raise AttributeError(\"You haven't setup the chatbot yet. \"\n",
        "                                 \"Setup the chatbot by calling the \"\n",
        "                                 \"instance method `setup_chatbot`.\")\n",
        "\n",
        "        answer = self.format_answer(ai_msg)\n",
        "\n",
        "        self.chat_history.save_context({'question':question},\n",
        "                                       {'answer':answer})\n",
        "\n",
        "        return answer"
      ],
      "metadata": {
        "id": "Dxnf--riyY12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osx_GOy6zt9O"
      },
      "source": [
        "### 6.4 Test our chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hat_DXPoPYfR"
      },
      "outputs": [],
      "source": [
        "chatbot = YouTubeChatbot(n_sources=3,\n",
        "                         k=5,\n",
        "                         timestamp_interval=datetime.timedelta(minutes=2),\n",
        "                         memory=5\n",
        "                        )\n",
        "\n",
        "url = 'https://www.youtube.com/watch?v=4Bdc55j80l8'\n",
        "chatbot.setup_chatbot(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDyc7EeVzzjj"
      },
      "source": [
        "#### 6.4.1 Test 1: A question that cannot be answered by the video alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1fTE8vfPYfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8b4686-f2b2-43a4-8af8-b30b714c3af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I cannot find the answer to that question in the video.\n"
          ]
        }
      ],
      "source": [
        "question = 'What is the capital city of Australia?'\n",
        "answer = chatbot.get_answer(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckevA0uW0Qsy"
      },
      "source": [
        "#### 6.4.2 Test 2: A question that can be answered by the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc7_m5u5PYfS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e9a372-7cf9-4827-8660-334cc7078a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers improved upon RNN in several ways. Firstly, transformers leverage the power of the attention mechanism to make better predictions, while RNNs suffer from short-term memory. This makes transformers better, especially if you want to encode or generate longer sequences. Secondly, because of the transformer architecture, the natural language processing industry can now achieve unprecedented results. You can find more information at these timestamps: 00:00:00, 00:03:02, 00:14:26.\n"
          ]
        }
      ],
      "source": [
        "question = 'In what ways did transformers improve upon RNN?'\n",
        "answer = chatbot.get_answer(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc90erEO1oyo"
      },
      "outputs": [],
      "source": [
        "del chatbot\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFHOyf8HPYfS"
      },
      "source": [
        "## 7. Web app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNvljS1mlBiZ"
      },
      "outputs": [],
      "source": [
        "class YouTubeChatbotApp(YouTubeChatbot):\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_sources: int,\n",
        "                 k: int,\n",
        "                 timestamp_interval: datetime.timedelta,\n",
        "                 memory: int,\n",
        "                 default_youtube_url: str\n",
        "                ):\n",
        "        super().__init__(n_sources, k, timestamp_interval, memory)\n",
        "        self.default_youtube_url = default_youtube_url\n",
        "        self.gradio_chat_history = []\n",
        "\n",
        "\n",
        "    def greet(self) -> list[tuple[str|None, str|None]]:\n",
        "        summary = get_summary(self.data)\n",
        "        summary_message = f'Here is a summary of the video \"{video_title}\":'\n",
        "        self.gradio_chat_history.append((None, summary_message))\n",
        "        self.gradio_chat_history.append((None, summary))\n",
        "        greeting_message = ('You can ask me anything about the video. '\n",
        "                            'I will do my best to answer!')\n",
        "        self.gradio_chat_history.append((None, greeting_message))\n",
        "        return self.gradio_chat_history\n",
        "\n",
        "\n",
        "    def question(self, user_message: str) -> list[tuple[str|None, str|None]]:\n",
        "        self.gradio_chat_history.append((user_message, None))\n",
        "        return '', self.gradio_chat_history\n",
        "\n",
        "\n",
        "    def respond(self) -> tuple[str, list[tuple[str|None, str|None]]]:\n",
        "        try:\n",
        "            ai_message = self.get_answer(self.gradio_chat_history[-1][0])\n",
        "        except AttributeError:\n",
        "            raise gr.Error('You need to process the video '\n",
        "                           'first by pressing the `Go` button.')\n",
        "\n",
        "\n",
        "        self.gradio_chat_history.append((None, ai_message))\n",
        "        return self.gradio_chat_history\n",
        "\n",
        "\n",
        "    def clear_chat_history(self) -> list:\n",
        "        self.chat_history.clear()\n",
        "        self.gradio_chat_history = []\n",
        "        return self.gradio_chat_history\n",
        "\n",
        "\n",
        "    def launch(self, **kwargs):\n",
        "\n",
        "        with gr.Blocks() as demo:\n",
        "\n",
        "            # Structure\n",
        "            with gr.Row():\n",
        "                url_input = gr.Textbox(value=self.default_youtube_url,\n",
        "                                      label='YouTube URL',\n",
        "                                      scale=5)\n",
        "                button = gr.Button(value='Go', scale=1)\n",
        "\n",
        "            chatbot = gr.Chatbot()\n",
        "            user_message = gr.Textbox(label='Ask a question:')\n",
        "            clear = gr.ClearButton([user_message, chatbot])\n",
        "\n",
        "\n",
        "            # Actions\n",
        "            button.click(self.clear_chat_history,\n",
        "                        inputs=[],\n",
        "                        outputs=[chatbot],\n",
        "                        trigger_mode='once'\n",
        "                        ).then(self.setup_chatbot,\n",
        "                                inputs=[url_input],\n",
        "                                outputs=[url_input]\n",
        "                        ).then(self.greet,\n",
        "                                inputs=[],\n",
        "                                outputs=[chatbot])\n",
        "\n",
        "            user_message.submit(self.question,\n",
        "                                inputs=[user_message],\n",
        "                                outputs=[user_message, chatbot]\n",
        "                                ).then(self.respond,\n",
        "                                      inputs=[],\n",
        "                                      outputs=[chatbot])\n",
        "\n",
        "            clear.click(self.clear_chat_history, inputs=[], outputs=[chatbot])\n",
        "\n",
        "\n",
        "        demo.launch(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app = YouTubeChatbotApp(n_sources=3,\n",
        "                        k=5,\n",
        "                        timestamp_interval=datetime.timedelta(minutes=2),\n",
        "                        memory=5,\n",
        "                        default_youtube_url='https://www.youtube.com/watch?v=4Bdc55j80l8'\n",
        "                       )\n",
        "\n",
        "app.launch(debug=True)"
      ],
      "metadata": {
        "id": "Ov8eA2-7u3LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XwNllRxxsVxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}