{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# YouTube Chatbot"
      ],
      "metadata": {
        "id": "JSe0VTTc6eWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes langchain chromadb jq \\\n",
        "sentence-transformers gradio yt_dlp\n",
        "\n",
        "!pip install -q git+https://github.com/m-bain/whisperx.git"
      ],
      "metadata": {
        "id": "2G5l5uag6iCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTsK_hF3PYfL",
        "outputId": "720af2c3-cfd1-40e2-ac0b-29edc9554019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import GenerationConfig\n",
        "from transformers import pipeline\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import JSONLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "import whisperx\n",
        "import yt_dlp\n",
        "import json\n",
        "import gc\n",
        "import gradio as gr\n",
        "import datetime\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGnYb4fhPYfL"
      },
      "source": [
        "## 1. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDbCV-HNPYfM"
      },
      "outputs": [],
      "source": [
        "audio_file_number = 1\n",
        "def yt_audio_to_text(url,\n",
        "                     device = \"cuda\",\n",
        "                     batch_size = 8,\n",
        "                     compute_type = \"float32\"\n",
        "                    ):\n",
        "    global audio_file_number\n",
        "    global progress\n",
        "    progress = gr.Progress()\n",
        "    progress(0.1)\n",
        "\n",
        "    with yt_dlp.YoutubeDL({'extract_audio': True,\n",
        "                           'format': 'bestaudio',\n",
        "                           'outtmpl': f'{audio_file_number}.mp3'}) as video:\n",
        "\n",
        "        info_dict = video.extract_info(url, download=False)\n",
        "        global video_title\n",
        "        video_title = info_dict['title']\n",
        "        video.download(url)\n",
        "\n",
        "    progress(0.3)\n",
        "    audio_file = f'{audio_file_number}.mp3'\n",
        "    audio_file_number += 1\n",
        "\n",
        "\n",
        "    model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "    progress(0.6)\n",
        "\n",
        "    audio = whisperx.load_audio(audio_file)\n",
        "\n",
        "    result = model.transcribe(audio, batch_size=batch_size)\n",
        "    progress(0.9)\n",
        "\n",
        "    with open('audio.json', 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    del model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syFJmVkJPYfM"
      },
      "source": [
        "## 2. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jjqkPjDPYfN"
      },
      "outputs": [],
      "source": [
        "def metadata_func(record: dict, metadata: dict) -> dict:\n",
        "\n",
        "    metadata['start'] = str(datetime.timedelta(seconds=round(record.get(\"start\"))))\n",
        "    metadata['end'] = str(datetime.timedelta(seconds=round(record.get(\"end\"))))\n",
        "    metadata['source'] =  metadata['start'] + '->' + metadata['end']\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    loader = JSONLoader(\n",
        "        file_path='audio.json',\n",
        "        jq_schema='.segments[]',\n",
        "        content_key='text',\n",
        "        metadata_func=metadata_func\n",
        "    )\n",
        "\n",
        "    global data\n",
        "    data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afNgaeFDPYfN"
      },
      "source": [
        "## 3. Create embeddings and vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e67FPeyiPYfN"
      },
      "outputs": [],
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name,\n",
        "                                   model_kwargs=model_kwargs)\n",
        "\n",
        "def create_vectordb(k):\n",
        "    global data\n",
        "    global vectordb\n",
        "    global retriever\n",
        "    vectordb = Chroma.from_documents(documents=data, embedding=embeddings)\n",
        "    retriever = vectordb.as_retriever(search_type=\"similarity\",\n",
        "                                      search_kwargs={\"k\": k})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9h-t8E9PYfO"
      },
      "source": [
        "## 4. Load LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkoefkhBPYfP"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"max_length\": 1024})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C65d6aO5ZQT8"
      },
      "source": [
        "## 5. Standalone question chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpsy_DGxPYfQ",
        "outputId": "8c981a4e-c5f3-4d4d-ae77-3233795b5a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no\n"
          ]
        }
      ],
      "source": [
        "standalone_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Given a chat history and the latest user question, \\\n",
        "    identify whether the question is a standalone question or the question \\\n",
        "    references the chat history. Answer 'yes' if the question is a standalone \\\n",
        "    question, and 'no' if the question references the chat history. Do not \\\n",
        "    answer anything other than 'yes' or 'no'.\n",
        "\n",
        "    Chat history:\n",
        "    {chat_history}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "def format_output(answer: str) -> str:\n",
        "    # All lower case and remove all whitespace\n",
        "    return ''.join(answer.lower().split())\n",
        "\n",
        "standalone_chain = standalone_prompt | llm | format_output\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "template = \"\"\"\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = prompt | llm\n",
        "\n",
        "\n",
        "# Test\n",
        "question = \"What is the capital of Australia?\"\n",
        "ai_msg = chain.invoke({'question':question})\n",
        "chat_history.extend([HumanMessage(content=question),\n",
        "                     AIMessage(content=ai_msg)])\n",
        "\n",
        "second_question = \"What is the population?\"\n",
        "answer = standalone_chain.invoke({'question': second_question,\n",
        "                                  'chat_history': chat_history})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQOv-Y0AnJSq"
      },
      "source": [
        "## 6. Condense question chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tix4I4iPYfQ",
        "outputId": "7513ad4f-3977-4737-d631-d115065b7895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    What is the population of Canberra?\n"
          ]
        }
      ],
      "source": [
        "condense_q_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"Given a chat history and the latest user question \\\n",
        "    which might reference the chat history, formulate a standalone question \\\n",
        "    which can be understood without the chat history. Do NOT answer the question, \\\n",
        "    just reformulate it if needed and otherwise return it as is.\n",
        "\n",
        "    Chat history: {chat_history}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "condense_q_chain = condense_q_prompt | llm\n",
        "\n",
        "\n",
        "# Test\n",
        "answer = condense_q_chain.invoke({'question': second_question,\n",
        "                                  'chat_history': chat_history})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyAN4SocnNm9"
      },
      "source": [
        "## 7. Q&A chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t8Rw-gtPYfR"
      },
      "outputs": [],
      "source": [
        "qa_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant for question-answering tasks. \\\n",
        "    Only use the following context to answer the question. \\\n",
        "    Do not answer with information that is not contained in \\\n",
        "    the context. If you don't know the answer, just say the \\\n",
        "    following in exact words: {no_answer_msg}.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "def format_docs(docs: list) -> str:\n",
        "    '''\n",
        "    Combine documents\n",
        "    '''\n",
        "    global sources\n",
        "    sources = [doc.metadata['start'] for doc in docs]\n",
        "\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "def standalone_question(input_: dict) -> str:\n",
        "    '''\n",
        "    If the question is a not a standalone question, run condense_q_chain\n",
        "    '''\n",
        "    if input_['standalone']=='yes':\n",
        "        return condense_q_chain\n",
        "    else:\n",
        "        return input_['question']\n",
        "\n",
        "\n",
        "def format_answer(answer: str,\n",
        "                  n_sources: int=1,\n",
        "                  timestamp_interval: datetime.timedelta=datetime.timedelta(minutes=5)) -> str:\n",
        "\n",
        "    no_whitespace_answer = ' '.join(answer.split())\n",
        "\n",
        "    if no_whitespace_answer == no_answer_msg:\n",
        "        return no_answer_msg\n",
        "    else:\n",
        "        timestamps = filter_timestamps(n_sources, timestamp_interval)\n",
        "        answer_with_sources = (answer.strip()\n",
        "        + ' You can find more information at these timestamps: {}.'.format(', '.join(timestamps))\n",
        "        )\n",
        "        return answer_with_sources\n",
        "\n",
        "\n",
        "def filter_timestamps(n_sources: int,\n",
        "                      timestamp_interval: datetime.timedelta=datetime.timedelta(minutes=5)) -> list:\n",
        "    '''Returns a list of timestamps with length `n_sources`.\n",
        "    The timestamps are at least an `timestamp_interval` apart.\n",
        "    '''\n",
        "    sorted_timestamps = sorted(sources)\n",
        "    output = [sorted_timestamps[0]]\n",
        "    i=1\n",
        "    while len(output)<n_sources:\n",
        "        timestamp1 = datetime.datetime.strptime(output[-1], '%H:%M:%S')\n",
        "\n",
        "        try:\n",
        "            timestamp2 = datetime.datetime.strptime(sorted_timestamps[i], '%H:%M:%S')\n",
        "        except IndexError:\n",
        "            break\n",
        "\n",
        "        time_diff = timestamp2 - timestamp1\n",
        "\n",
        "        if time_diff>timestamp_interval:\n",
        "            output.append(str(timestamp2.time()))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def setup_rag(url):\n",
        "    global retriever\n",
        "    global rag_chain\n",
        "\n",
        "    yt_audio_to_text(url)\n",
        "    load_data()\n",
        "    create_vectordb(k)\n",
        "\n",
        "    rag_chain = (\n",
        "        RunnablePassthrough.assign(standalone=standalone_chain)\n",
        "        | {'question':standalone_question,\n",
        "           'context':standalone_question|retriever|format_docs,\n",
        "           'no_answer_msg':lambda input_: input_['no_answer_msg']\n",
        "          }\n",
        "        | qa_prompt\n",
        "        | llm\n",
        "    )\n",
        "\n",
        "    return url\n",
        "\n",
        "\n",
        "\n",
        "def get_answer(question: str) -> str:\n",
        "    global chat_history\n",
        "    global rag_chain\n",
        "    ai_msg = rag_chain.invoke({\"question\": question,\n",
        "                               \"chat_history\": chat_history,\n",
        "                               \"no_answer_msg\": no_answer_msg\n",
        "                              })\n",
        "\n",
        "    answer = format_answer(ai_msg, n_sources, timestamp_interval)\n",
        "\n",
        "    chat_history.extend([HumanMessage(content=question), AIMessage(content=answer)])\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Test our chatbot"
      ],
      "metadata": {
        "id": "osx_GOy6zt9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hat_DXPoPYfR"
      },
      "outputs": [],
      "source": [
        "no_answer_msg = \"Sorry, I cannot find the answer to that question in the video.\"\n",
        "n_sources = 3 # Number of sources provided in the answer\n",
        "k = 10 # Number of documents returned by the retriever\n",
        "timestamp_interval = datetime.timedelta(minutes=5)\n",
        "chat_history = []\n",
        "url = 'https://www.youtube.com/watch?v=SZorAJ4I-sA'\n",
        "setup_rag(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Test 1: A question that cannot be answered by the video alone."
      ],
      "metadata": {
        "id": "NDyc7EeVzzjj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1fTE8vfPYfR",
        "outputId": "bb621759-ea71-46c3-8965-8010ed04bb09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sorry, I cannot find the answer to that question in the video.\n"
          ]
        }
      ],
      "source": [
        "question = \"Where is Canberra?\"\n",
        "answer = get_answer(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Test 2: A question that can be answered by the video."
      ],
      "metadata": {
        "id": "ckevA0uW0Qsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc7_m5u5PYfS",
        "outputId": "ed80b597-611d-4242-8ef0-5ac768e82420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A decoder is the second part of a transformer that starts with word embedding and creates embedding values for the output vocabulary, which consists of the Spanish words, IR, VAMOS, E, and the EOS end-of-sequence token. It allows the encoder-decoder attention to focus on the relationships between the output words and the input words without having to preserve the self-attention or word and position encoding that happened earlier. It also uses a fully connected layer to You can find more information here: 0:23:30, 00:31:05.\n"
          ]
        }
      ],
      "source": [
        "question = \"What are transformers good at?\"\n",
        "answer = get_answer(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del rag_chain, retriever, vectordb, data\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "hc90erEO1oyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFHOyf8HPYfS"
      },
      "source": [
        "## 9. Web app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eef-j5rtktla"
      },
      "outputs": [],
      "source": [
        "# Chatbot settings\n",
        "no_answer_msg = \"Sorry, I cannot find the answer to that question in the video.\"\n",
        "n_sources = 3 # Number of sources provided in the answer\n",
        "k = 10 # Number of documents returned by the retriever\n",
        "timestamp_interval = datetime.timedelta(minutes=3)\n",
        "default_youtube_url = 'https://www.youtube.com/watch?v=SZorAJ4I-sA'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G-gjmC7PYfS"
      },
      "outputs": [],
      "source": [
        "def greet():\n",
        "    global gradio_chat_history\n",
        "    greeting_message = f'You can ask me anything about the video \"{video_title}\". I will do my best to answer!'\n",
        "    gradio_chat_history.append((None, greeting_message))\n",
        "    return gradio_chat_history\n",
        "\n",
        "def respond(message):\n",
        "    global gradio_chat_history\n",
        "    ai_message = get_answer(message)\n",
        "    gradio_chat_history.append((message, ai_message))\n",
        "    return \"\", gradio_chat_history\n",
        "\n",
        "def clear_chat_history():\n",
        "    global chat_history\n",
        "    global gradio_chat_history\n",
        "    chat_history=[]\n",
        "    gradio_chat_history=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "UNvljS1mlBiZ",
        "outputId": "ded78d2a-c271-4e55-e548-fa862c2a9b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://348f24a01c836bd6e3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://348f24a01c836bd6e3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=SZorAJ4I-sA\n",
            "[youtube] SZorAJ4I-sA: Downloading webpage\n",
            "[youtube] SZorAJ4I-sA: Downloading ios player API JSON\n",
            "[youtube] SZorAJ4I-sA: Downloading android player API JSON\n",
            "[youtube] SZorAJ4I-sA: Downloading m3u8 information\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=SZorAJ4I-sA\n",
            "[youtube] SZorAJ4I-sA: Downloading webpage\n",
            "[youtube] SZorAJ4I-sA: Downloading ios player API JSON\n",
            "[youtube] SZorAJ4I-sA: Downloading android player API JSON\n",
            "[youtube] SZorAJ4I-sA: Downloading m3u8 information\n",
            "[info] SZorAJ4I-sA: Downloading 1 format(s): 251\n",
            "[download] Destination: 4.mp3\n",
            "[download] 100% of    8.61MiB in 00:00:00 at 26.23MiB/s  \n",
            "No language specified, language will be first be detected for each audio file (increases inference time).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (1.00) in first 30s of audio...\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://348f24a01c836bd6e3.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "chat_history = []\n",
        "gradio_chat_history = []\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    # Structure\n",
        "    with gr.Row():\n",
        "        url_input = gr.Textbox(value=default_youtube_url,\n",
        "                               label='YouTube URL',\n",
        "                               scale=5)\n",
        "        button = gr.Button(value='Go', scale=1)\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    user_message = gr.Textbox(label='Ask a question:')\n",
        "    clear = gr.ClearButton([user_message, chatbot])\n",
        "\n",
        "\n",
        "    # Actions\n",
        "    button.click(setup_rag,\n",
        "                 inputs=[url_input],\n",
        "                 outputs=[url_input],\n",
        "                 trigger_mode='once').then(greet,\n",
        "                                           inputs=[],\n",
        "                                           outputs=[chatbot])\n",
        "\n",
        "    user_message.submit(respond,\n",
        "                        inputs=[user_message],\n",
        "                        outputs=[user_message, chatbot])\n",
        "\n",
        "    clear.click(clear_chat_history)\n",
        "\n",
        "\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yHBJBzL5RZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}